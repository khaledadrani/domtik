{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJpXpmjEYC_T"
   },
   "source": [
    "## Building a GPT\n",
    "\n",
    "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mk3WGYcsqWsb"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uWoUyt3YsmIc",
    "outputId": "e7985984-62dd-45cf-9798-fb8983d040e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#google colab\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffkC2D64qbFJ",
    "outputId": "9bc2f984-63d3-419a-e649-86cf741bacbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Obtaining dependency information for pymupdf from https://files.pythonhosted.org/packages/84/84/9373889332f6136be853f0e10f2cec2bc19149ca888bbd10dfe9e6183963/PyMuPDF-1.24.2-cp310-none-win_amd64.whl.metadata\n",
      "  Downloading PyMuPDF-1.24.2-cp310-none-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting PyMuPDFb==1.24.1 (from pymupdf)\n",
      "  Obtaining dependency information for PyMuPDFb==1.24.1 from https://files.pythonhosted.org/packages/7e/e9/d7eb31501a28dd4579b912847187f49bfdeaf18e2d408aa3b2401606c45c/PyMuPDFb-1.24.1-py3-none-win_amd64.whl.metadata\n",
      "  Downloading PyMuPDFb-1.24.1-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Downloading PyMuPDF-1.24.2-cp310-none-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.2 MB 1.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.1/3.2 MB 1.1 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.1/3.2 MB 819.2 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/3.2 MB 952.6 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/3.2 MB 952.6 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/3.2 MB 952.6 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/3.2 MB 952.6 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.2/3.2 MB 687.0 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.3/3.2 MB 846.5 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.4/3.2 MB 849.3 kB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 0.4/3.2 MB 865.0 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.4/3.2 MB 860.2 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.5/3.2 MB 879.9 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.5/3.2 MB 856.1 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.6/3.2 MB 905.4 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.6/3.2 MB 905.4 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.6/3.2 MB 905.4 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.6/3.2 MB 905.4 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.6/3.2 MB 905.4 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.6/3.2 MB 905.4 kB/s eta 0:00:03\n",
      "   ------- -------------------------------- 0.6/3.2 MB 655.3 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.7/3.2 MB 654.9 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.7/3.2 MB 635.7 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.7/3.2 MB 635.7 kB/s eta 0:00:04\n",
      "   -------- ------------------------------- 0.7/3.2 MB 626.9 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 0.7/3.2 MB 593.5 kB/s eta 0:00:05\n",
      "   -------- ------------------------------- 0.7/3.2 MB 594.7 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.7/3.2 MB 581.6 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.7/3.2 MB 568.5 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.8/3.2 MB 563.8 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.8/3.2 MB 563.8 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.8/3.2 MB 563.8 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.8/3.2 MB 563.8 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 0.8/3.2 MB 515.0 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.8/3.2 MB 507.5 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.8/3.2 MB 507.5 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.8/3.2 MB 490.0 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.8/3.2 MB 490.0 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.9/3.2 MB 481.3 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.9/3.2 MB 481.3 kB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 0.9/3.2 MB 464.2 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 0.9/3.2 MB 463.5 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.9/3.2 MB 454.0 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 0.9/3.2 MB 457.1 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.9/3.2 MB 457.1 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.9/3.2 MB 457.1 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.9/3.2 MB 457.1 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 0.9/3.2 MB 429.0 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 0.9/3.2 MB 429.0 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 1.0/3.2 MB 436.9 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 1.0/3.2 MB 436.9 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 1.0/3.2 MB 436.9 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 1.0/3.2 MB 436.9 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 1.0/3.2 MB 436.9 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 1.0/3.2 MB 401.4 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 1.0/3.2 MB 401.4 kB/s eta 0:00:06\n",
      "   ------------ --------------------------- 1.0/3.2 MB 401.4 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.0/3.2 MB 396.3 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.0/3.2 MB 396.3 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.1/3.2 MB 387.9 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.1/3.2 MB 387.9 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.1/3.2 MB 382.9 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.1/3.2 MB 382.9 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.1/3.2 MB 382.9 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.1/3.2 MB 371.5 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.1/3.2 MB 371.0 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.1/3.2 MB 371.0 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.1/3.2 MB 366.3 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.1/3.2 MB 366.3 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.1/3.2 MB 363.7 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 358.1 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 358.1 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 355.8 kB/s eta 0:00:06\n",
      "   -------------- ------------------------- 1.2/3.2 MB 288.8 kB/s eta 0:00:07\n",
      "   --------------- ------------------------ 1.3/3.2 MB 305.3 kB/s eta 0:00:07\n",
      "   --------------- ------------------------ 1.3/3.2 MB 305.3 kB/s eta 0:00:07\n",
      "   --------------- ------------------------ 1.3/3.2 MB 305.3 kB/s eta 0:00:07\n",
      "   --------------- ------------------------ 1.3/3.2 MB 305.3 kB/s eta 0:00:07\n",
      "   --------------- ------------------------ 1.3/3.2 MB 305.3 kB/s eta 0:00:07\n",
      "   --------------- ------------------------ 1.3/3.2 MB 305.3 kB/s eta 0:00:07\n",
      "   --------------- ------------------------ 1.3/3.2 MB 305.3 kB/s eta 0:00:07\n",
      "   --------------- ------------------------ 1.3/3.2 MB 305.3 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 286.4 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 286.4 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 286.4 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 283.1 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 283.1 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 283.1 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 283.1 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 283.1 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 283.1 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 270.6 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 270.6 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 270.6 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 270.6 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 270.6 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 270.6 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 270.6 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 270.6 kB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 255.9 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 255.9 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 1.3/3.2 MB 255.9 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 1.4/3.2 MB 251.8 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 1.4/3.2 MB 251.8 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 1.4/3.2 MB 251.8 kB/s eta 0:00:08\n",
      "   ---------------- ----------------------- 1.4/3.2 MB 251.8 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 246.9 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 246.9 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 246.9 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 245.1 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 245.1 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 242.8 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 242.8 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 242.8 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 241.8 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 240.4 kB/s eta 0:00:08\n",
      "   ----------------- ---------------------- 1.4/3.2 MB 240.4 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 1.5/3.2 MB 240.6 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 1.5/3.2 MB 240.6 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 1.5/3.2 MB 240.3 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 1.5/3.2 MB 240.3 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 1.5/3.2 MB 238.9 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 1.5/3.2 MB 238.9 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 1.5/3.2 MB 239.2 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 1.5/3.2 MB 239.0 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 1.5/3.2 MB 239.0 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 1.5/3.2 MB 239.0 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 1.5/3.2 MB 239.0 kB/s eta 0:00:08\n",
      "   ------------------ --------------------- 1.5/3.2 MB 239.0 kB/s eta 0:00:08\n",
      "   ------------------- -------------------- 1.6/3.2 MB 237.9 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 1.6/3.2 MB 237.9 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 1.6/3.2 MB 234.9 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 1.6/3.2 MB 234.9 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 1.6/3.2 MB 234.9 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 1.6/3.2 MB 234.7 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 1.6/3.2 MB 234.7 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 1.6/3.2 MB 233.0 kB/s eta 0:00:07\n",
      "   ------------------- -------------------- 1.6/3.2 MB 233.0 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 1.6/3.2 MB 232.3 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 1.6/3.2 MB 232.3 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 1.6/3.2 MB 232.3 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 1.6/3.2 MB 232.3 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 1.6/3.2 MB 229.0 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 1.6/3.2 MB 229.4 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 1.6/3.2 MB 229.4 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 1.6/3.2 MB 229.4 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 1.7/3.2 MB 227.8 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 1.7/3.2 MB 227.8 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 1.7/3.2 MB 225.4 kB/s eta 0:00:07\n",
      "   -------------------- ------------------- 1.7/3.2 MB 225.4 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.7/3.2 MB 225.7 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.7/3.2 MB 225.7 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.7/3.2 MB 225.7 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.7/3.2 MB 224.7 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.7/3.2 MB 224.7 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.7/3.2 MB 224.7 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.7/3.2 MB 224.7 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.7/3.2 MB 224.7 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.8/3.2 MB 222.8 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.8/3.2 MB 222.8 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.8/3.2 MB 222.8 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.8/3.2 MB 220.2 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.8/3.2 MB 220.2 kB/s eta 0:00:07\n",
      "   --------------------- ------------------ 1.8/3.2 MB 220.2 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 219.3 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 219.3 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 219.3 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 219.3 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 219.3 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 216.0 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 216.0 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 215.2 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 215.2 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 215.2 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 215.2 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 212.9 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 212.9 kB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 1.8/3.2 MB 212.9 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.8/3.2 MB 210.2 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.8/3.2 MB 210.2 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.8/3.2 MB 210.2 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 209.6 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 209.6 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 209.6 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 209.6 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 207.8 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 207.8 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 207.8 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 207.8 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 207.8 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 207.8 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 203.7 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 203.7 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 203.7 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 203.7 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 202.5 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 202.5 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 202.5 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 202.5 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 202.5 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 202.5 kB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 1.9/3.2 MB 202.5 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 1.9/3.2 MB 196.7 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 1.9/3.2 MB 196.7 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 1.9/3.2 MB 196.7 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 1.9/3.2 MB 195.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 1.9/3.2 MB 195.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 1.9/3.2 MB 195.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 1.9/3.2 MB 195.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 1.9/3.2 MB 195.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 193.5 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 193.5 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 193.5 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 193.5 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 193.5 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 190.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 190.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 190.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 190.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 188.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 188.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 188.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 188.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 188.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 188.4 kB/s eta 0:00:07\n",
      "   ------------------------ --------------- 2.0/3.2 MB 188.4 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 184.1 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 184.1 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 184.1 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 184.1 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 184.1 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 184.1 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 184.1 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 180.6 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 180.6 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 180.6 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 180.6 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 180.6 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 178.4 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 178.4 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 178.4 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 178.4 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 178.4 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.0/3.2 MB 178.4 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.1/3.2 MB 175.0 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.1/3.2 MB 175.0 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.1/3.2 MB 175.0 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.1/3.2 MB 175.0 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.1/3.2 MB 175.0 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.1/3.2 MB 175.0 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.1/3.2 MB 172.4 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.1/3.2 MB 172.4 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.1/3.2 MB 172.4 kB/s eta 0:00:07\n",
      "   ------------------------- -------------- 2.1/3.2 MB 172.4 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 170.3 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 170.3 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 170.3 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 170.3 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 170.3 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 170.3 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 168.6 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 168.6 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 168.6 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 168.6 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 167.7 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 167.7 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 166.6 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 166.6 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 166.6 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.1/3.2 MB 166.6 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.2/3.2 MB 166.2 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.2/3.2 MB 166.2 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 2.2/3.2 MB 166.2 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 165.2 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 165.2 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 165.2 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 165.2 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 164.0 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 164.0 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 164.0 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 164.0 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 163.3 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 163.3 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 163.3 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 163.3 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 162.0 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 2.2/3.2 MB 162.0 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 2.2/3.2 MB 162.2 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.2/3.2 MB 162.2 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.2/3.2 MB 162.2 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.2/3.2 MB 162.2 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 160.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 160.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 160.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 160.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 160.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 160.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 160.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 159.0 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 159.0 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 159.0 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 159.0 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 158.2 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 158.2 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 158.2 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 158.2 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 158.2 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 158.2 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 155.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 155.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 155.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 155.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 155.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 155.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 155.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 155.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 155.5 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 2.3/3.2 MB 155.5 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 152.2 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 152.2 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 152.2 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 152.2 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 152.2 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 152.2 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 150.1 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 150.1 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 150.1 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 150.1 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 150.1 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 150.1 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 150.1 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 150.1 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.3/3.2 MB 150.1 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 147.6 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 147.6 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 147.6 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 147.6 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 147.6 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 146.4 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 146.4 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 146.4 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 146.4 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 146.4 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 146.4 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 146.4 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 144.4 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 144.4 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 144.4 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 144.4 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 2.4/3.2 MB 144.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 143.5 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 143.5 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 143.5 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 143.5 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 143.5 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 143.5 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 143.5 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 143.5 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 141.0 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 141.0 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 141.0 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 141.0 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 141.0 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 141.0 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 141.0 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 141.0 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 141.0 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 141.0 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.4/3.2 MB 138.4 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 133.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 133.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 133.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 133.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 133.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 133.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 133.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 133.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 133.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 133.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 133.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 133.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 133.8 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 2.5/3.2 MB 130.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 122.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 118.5 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 118.5 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 118.5 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 118.5 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 118.5 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 118.5 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 117.1 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 117.6 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 117.6 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 117.6 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 117.7 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 117.7 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 117.7 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 2.5/3.2 MB 117.5 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 2.6/3.2 MB 118.1 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 2.6/3.2 MB 118.1 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 2.6/3.2 MB 118.2 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 2.6/3.2 MB 118.8 kB/s eta 0:00:06\n",
      "   -------------------------------- ------- 2.6/3.2 MB 119.4 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 2.6/3.2 MB 119.4 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 2.6/3.2 MB 119.5 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 2.7/3.2 MB 120.2 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 2.7/3.2 MB 120.3 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 2.7/3.2 MB 121.0 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 2.7/3.2 MB 121.6 kB/s eta 0:00:05\n",
      "   --------------------------------- ------ 2.7/3.2 MB 121.8 kB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 2.7/3.2 MB 122.4 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 2.7/3.2 MB 122.4 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 2.8/3.2 MB 122.9 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 2.8/3.2 MB 123.6 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 2.8/3.2 MB 124.6 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 2.8/3.2 MB 125.0 kB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 2.9/3.2 MB 126.3 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 2.9/3.2 MB 126.6 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 2.9/3.2 MB 127.7 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 2.9/3.2 MB 128.4 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 2.9/3.2 MB 129.4 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 3.0/3.2 MB 130.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 3.0/3.2 MB 131.2 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 3.0/3.2 MB 132.2 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 3.1/3.2 MB 133.3 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 3.1/3.2 MB 134.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.1/3.2 MB 135.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------  3.2/3.2 MB 136.9 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 138.4 kB/s eta 0:00:00\n",
      "Downloading PyMuPDFb-1.24.1-py3-none-win_amd64.whl (24.9 MB)\n",
      "   ---------------------------------------- 0.0/24.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/24.9 MB 1.3 MB/s eta 0:00:20\n",
      "   ---------------------------------------- 0.1/24.9 MB 812.7 kB/s eta 0:00:31\n",
      "   ---------------------------------------- 0.1/24.9 MB 744.7 kB/s eta 0:00:34\n",
      "   ---------------------------------------- 0.1/24.9 MB 798.9 kB/s eta 0:00:32\n",
      "   ---------------------------------------- 0.2/24.9 MB 817.0 kB/s eta 0:00:31\n",
      "   ---------------------------------------- 0.2/24.9 MB 748.1 kB/s eta 0:00:34\n",
      "   ---------------------------------------- 0.2/24.9 MB 687.0 kB/s eta 0:00:36\n",
      "   ---------------------------------------- 0.3/24.9 MB 714.4 kB/s eta 0:00:35\n",
      "   ---------------------------------------- 0.3/24.9 MB 737.3 kB/s eta 0:00:34\n",
      "    --------------------------------------- 0.3/24.9 MB 748.1 kB/s eta 0:00:33\n",
      "    --------------------------------------- 0.4/24.9 MB 739.0 kB/s eta 0:00:34\n",
      "    --------------------------------------- 0.4/24.9 MB 769.9 kB/s eta 0:00:32\n",
      "    --------------------------------------- 0.5/24.9 MB 760.5 kB/s eta 0:00:33\n",
      "    --------------------------------------- 0.5/24.9 MB 789.0 kB/s eta 0:00:31\n",
      "    --------------------------------------- 0.5/24.9 MB 779.3 kB/s eta 0:00:32\n",
      "    --------------------------------------- 0.6/24.9 MB 800.0 kB/s eta 0:00:31\n",
      "    --------------------------------------- 0.6/24.9 MB 788.7 kB/s eta 0:00:31\n",
      "   - -------------------------------------- 0.7/24.9 MB 806.1 kB/s eta 0:00:31\n",
      "   - -------------------------------------- 0.7/24.9 MB 812.9 kB/s eta 0:00:30\n",
      "   - -------------------------------------- 0.7/24.9 MB 812.9 kB/s eta 0:00:30\n",
      "   - -------------------------------------- 0.8/24.9 MB 826.5 kB/s eta 0:00:30\n",
      "   - -------------------------------------- 0.8/24.9 MB 852.5 kB/s eta 0:00:29\n",
      "   - -------------------------------------- 0.9/24.9 MB 863.3 kB/s eta 0:00:28\n",
      "   - -------------------------------------- 0.9/24.9 MB 851.8 kB/s eta 0:00:29\n",
      "   - -------------------------------------- 1.0/24.9 MB 873.8 kB/s eta 0:00:28\n",
      "   - -------------------------------------- 1.0/24.9 MB 882.5 kB/s eta 0:00:28\n",
      "   - -------------------------------------- 1.1/24.9 MB 890.4 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 1.1/24.9 MB 892.9 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 1.2/24.9 MB 889.1 kB/s eta 0:00:27\n",
      "   - -------------------------------------- 1.2/24.9 MB 880.9 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 1.3/24.9 MB 878.5 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 1.3/24.9 MB 880.3 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 1.3/24.9 MB 887.5 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 1.4/24.9 MB 893.6 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 1.4/24.9 MB 892.9 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 1.5/24.9 MB 898.6 kB/s eta 0:00:27\n",
      "   -- ------------------------------------- 1.5/24.9 MB 903.9 kB/s eta 0:00:26\n",
      "   -- ------------------------------------- 1.6/24.9 MB 908.9 kB/s eta 0:00:26\n",
      "   -- ------------------------------------- 1.6/24.9 MB 913.7 kB/s eta 0:00:26\n",
      "   -- ------------------------------------- 1.7/24.9 MB 912.6 kB/s eta 0:00:26\n",
      "   -- ------------------------------------- 1.7/24.9 MB 917.5 kB/s eta 0:00:26\n",
      "   -- ------------------------------------- 1.8/24.9 MB 921.7 kB/s eta 0:00:26\n",
      "   -- ------------------------------------- 1.8/24.9 MB 925.7 kB/s eta 0:00:25\n",
      "   --- ------------------------------------ 1.9/24.9 MB 929.5 kB/s eta 0:00:25\n",
      "   --- ------------------------------------ 1.9/24.9 MB 930.5 kB/s eta 0:00:25\n",
      "   --- ------------------------------------ 2.0/24.9 MB 931.8 kB/s eta 0:00:25\n",
      "   --- ------------------------------------ 2.0/24.9 MB 935.2 kB/s eta 0:00:25\n",
      "   --- ------------------------------------ 2.1/24.9 MB 938.5 kB/s eta 0:00:25\n",
      "   --- ------------------------------------ 2.1/24.9 MB 942.1 kB/s eta 0:00:25\n",
      "   --- ------------------------------------ 2.2/24.9 MB 940.6 kB/s eta 0:00:25\n",
      "   --- ------------------------------------ 2.2/24.9 MB 949.7 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 2.3/24.9 MB 952.9 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 2.3/24.9 MB 955.5 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 2.4/24.9 MB 958.1 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 2.4/24.9 MB 956.4 kB/s eta 0:00:24\n",
      "   --- ------------------------------------ 2.5/24.9 MB 956.9 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 2.5/24.9 MB 963.5 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 2.6/24.9 MB 967.3 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 2.6/24.9 MB 969.5 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 2.7/24.9 MB 958.8 kB/s eta 0:00:24\n",
      "   ---- ----------------------------------- 2.7/24.9 MB 971.7 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 2.8/24.9 MB 968.6 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 2.8/24.9 MB 970.5 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 2.9/24.9 MB 972.4 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 2.9/24.9 MB 970.8 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 3.0/24.9 MB 972.7 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 3.0/24.9 MB 974.4 kB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 3.1/24.9 MB 976.5 kB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 3.1/24.9 MB 981.4 kB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 3.2/24.9 MB 983.0 kB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 3.2/24.9 MB 984.5 kB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 3.3/24.9 MB 986.0 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 3.3/24.9 MB 984.4 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 3.4/24.9 MB 985.8 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 3.4/24.9 MB 985.9 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 3.5/24.9 MB 988.9 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 3.5/24.9 MB 990.2 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 3.6/24.9 MB 992.8 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 3.6/24.9 MB 994.3 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 3.7/24.9 MB 995.6 kB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 3.7/24.9 MB 996.7 kB/s eta 0:00:22\n",
      "   ------ --------------------------------- 3.7/24.9 MB 996.7 kB/s eta 0:00:22\n",
      "   ------ --------------------------------- 3.8/24.9 MB 996.3 kB/s eta 0:00:22\n",
      "   ------ --------------------------------- 3.9/24.9 MB 997.5 kB/s eta 0:00:22\n",
      "   ------ --------------------------------- 3.9/24.9 MB 998.6 kB/s eta 0:00:22\n",
      "   ------ --------------------------------- 4.0/24.9 MB 999.7 kB/s eta 0:00:21\n",
      "   ------ --------------------------------- 4.0/24.9 MB 1.0 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 4.1/24.9 MB 1.0 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 4.1/24.9 MB 1.0 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 4.2/24.9 MB 1.0 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 4.2/24.9 MB 1.0 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 4.2/24.9 MB 1.0 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 4.2/24.9 MB 1.0 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 4.4/24.9 MB 1.0 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 4.4/24.9 MB 1.0 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 4.5/24.9 MB 1.0 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 4.5/24.9 MB 1.0 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 4.6/24.9 MB 1.0 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 4.6/24.9 MB 1.0 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 4.7/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 4.7/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 4.8/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 4.8/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 4.8/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 4.9/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   ------- -------------------------------- 4.9/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 5.0/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 5.0/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 5.1/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 5.2/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 5.2/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 5.3/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 5.3/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 5.3/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 5.4/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 5.4/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 5.5/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 5.5/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 5.6/24.9 MB 1.0 MB/s eta 0:00:20\n",
      "   --------- ------------------------------ 5.6/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 5.7/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 5.7/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 5.8/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 5.8/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 5.9/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 5.9/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 6.0/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 6.0/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 6.1/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 6.1/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 6.2/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 6.2/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 6.3/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 6.3/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 6.4/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 6.4/24.9 MB 1.0 MB/s eta 0:00:19\n",
      "   ---------- ----------------------------- 6.5/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 6.5/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 6.6/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 6.6/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 6.7/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 6.7/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 6.8/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 6.8/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 6.9/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 6.9/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 7.0/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 7.0/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 7.1/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 7.1/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 7.2/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 7.2/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 7.3/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 7.3/24.9 MB 1.0 MB/s eta 0:00:18\n",
      "   ----------- ---------------------------- 7.4/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 7.4/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 7.5/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 7.5/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 7.6/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 7.6/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 7.7/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 7.7/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 7.8/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 7.8/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 7.8/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 7.8/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 8.0/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 8.0/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 8.1/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 8.1/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 8.2/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 8.2/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 8.3/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 8.3/24.9 MB 1.0 MB/s eta 0:00:17\n",
      "   ------------- -------------------------- 8.4/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 8.4/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 8.5/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 8.5/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 8.5/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 8.6/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 8.7/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 8.7/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 8.8/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 8.8/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 8.9/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 8.9/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 9.0/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 9.0/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 9.1/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 9.1/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 9.2/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 9.2/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 9.3/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   -------------- ------------------------- 9.3/24.9 MB 1.0 MB/s eta 0:00:16\n",
      "   --------------- ------------------------ 9.3/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.4/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.4/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.5/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.5/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.6/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.6/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.7/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.7/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.8/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.9/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 9.9/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   --------------- ------------------------ 10.0/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 10.0/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 10.0/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 10.1/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 10.1/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 10.2/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 10.2/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 10.3/24.9 MB 1.0 MB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 10.3/24.9 MB 1.0 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 10.3/24.9 MB 1.0 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 10.4/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 10.5/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 10.5/24.9 MB 1.0 MB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 10.6/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 10.6/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 10.7/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 10.7/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 10.8/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 10.8/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 10.9/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 10.9/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 10.9/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 11.0/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 11.0/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 11.1/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 11.1/24.9 MB 1.1 MB/s eta 0:00:14\n",
      "   ----------------- ---------------------- 11.2/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.2/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.3/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.3/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.4/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.4/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.5/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.6/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.6/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.6/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.7/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.8/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 11.8/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 11.8/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 11.9/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 11.9/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 12.0/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 12.1/24.9 MB 1.1 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 12.1/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 12.2/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 12.2/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 12.2/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 12.3/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 12.3/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   ------------------- -------------------- 12.4/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 12.5/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 12.5/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 12.5/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 12.6/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 12.6/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 12.7/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 12.7/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 12.8/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 12.9/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 12.9/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 13.0/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 13.0/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 13.1/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 13.1/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 13.1/24.9 MB 1.1 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 13.2/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 13.3/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 13.3/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 13.4/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 13.4/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 13.5/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 13.5/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 13.5/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 13.6/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   --------------------- ------------------ 13.6/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 13.7/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 13.7/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 13.8/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 13.9/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 13.9/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 14.0/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 14.0/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 14.0/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 14.1/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 14.2/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 14.2/24.9 MB 1.1 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 14.3/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.3/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.3/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.4/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.4/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.5/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.6/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.6/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.7/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.7/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.7/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.7/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.9/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 14.9/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 14.9/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 15.0/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 15.0/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 15.1/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 15.1/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 15.2/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 15.2/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 15.3/24.9 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------ --------------- 15.3/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 15.4/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 15.4/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 15.5/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 15.5/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 15.6/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 15.6/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 15.7/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 15.7/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 15.8/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 15.9/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 15.9/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 16.0/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 16.0/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 16.1/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 16.1/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 16.1/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 16.2/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 16.3/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 16.3/24.9 MB 1.1 MB/s eta 0:00:09\n",
      "   -------------------------- ------------- 16.4/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 16.4/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 16.5/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 16.5/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 16.6/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 16.6/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 16.7/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 16.7/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 16.8/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 16.8/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 16.9/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 16.9/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 17.0/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 17.0/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 17.1/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 17.1/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 17.2/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 17.2/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 17.3/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 17.3/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 17.3/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------------------- ------------ 17.3/24.9 MB 1.1 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 17.5/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 17.5/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 17.6/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 17.6/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 17.7/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 17.7/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 17.8/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 17.8/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 17.9/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 17.9/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 18.0/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 18.0/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 18.1/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 18.1/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 18.2/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 18.2/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 18.3/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 18.3/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 18.3/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 18.4/24.9 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 18.5/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 18.5/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 18.6/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 18.6/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 18.7/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 18.7/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 18.8/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 18.8/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 18.9/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 18.9/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 19.0/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 19.0/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 19.1/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 19.1/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 19.2/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 19.2/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 19.3/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 19.3/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 19.4/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 19.4/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 19.4/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 19.5/24.9 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 19.6/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 19.6/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 19.7/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 19.7/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 19.8/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 19.8/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 19.8/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 19.9/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 19.9/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.0/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.1/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.1/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.2/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.2/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.3/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.3/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.4/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.4/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.5/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 20.5/24.9 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 20.6/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 20.6/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 20.7/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 20.7/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 20.8/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 20.8/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 20.8/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 20.9/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 21.0/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 21.0/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 21.1/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 21.1/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.2/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.2/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.3/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.3/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.4/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.4/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.5/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.5/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.6/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.6/24.9 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 21.7/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 21.7/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 21.8/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 21.8/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 21.9/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 21.9/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.0/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.0/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.1/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.1/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.2/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.2/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.3/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.3/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 22.4/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.4/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.5/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.5/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.6/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.6/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.7/24.9 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 22.8/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.8/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.9/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 22.9/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 23.0/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 23.0/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 23.0/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.1/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.1/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.2/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.3/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.3/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.4/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.4/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.5/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.5/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 23.6/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.7/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.7/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.8/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.8/24.9 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 23.8/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 23.9/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.0/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.0/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.0/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.1/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.1/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.2/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 24.2/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.3/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.4/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.5/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.5/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.7/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.7/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.7/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.8/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/24.9 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.9/24.9 MB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: PyMuPDFb, pymupdf\n",
      "Successfully installed PyMuPDFb-1.24.1 pymupdf-1.24.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gJfDG_Pqb6D"
   },
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V51Qixs_stU6"
   },
   "source": [
    "## local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aQKaeMSSsrGi"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "main_path = \"/content/drive/MyDrive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgFInSFvtRnZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ccniuVqztiWa",
    "outputId": "e80d97c3-db4d-4ca8-90f0-4a89775d7a7f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(841041,\n",
       " '\\x0c \\nHow Linux Works: What Every Superuser Should Know \\nBrian Ward \\nPublished by No Starch Press \\n\\x0cPraise for the First Edition of How Linux Works \\nA great resource. In roughly 350 pages, the book covers all the basics. \\nEWEEK \\nI would definitely recommend this book to those who are interested in Linux, but have not had the experience \\nto know the inner workings of the OS. \\nOREILLYNET \\nOne of the best basic books on learning Linux, written with the power user in mind. Five stars. \\nOPENSOURCE-BOOK-REVIEWS.COM \\nSucceeds admirably because of the way in which its organized and the level of technical detail it offers. \\nKICKSTART NEWS \\nThis is a very different introduction to Linux. Its unflashy, concentrates on the command line, and digs \\naround in the internals rather than on GUI frontends that take the place of more familiar MS Windows tools. \\nTECHBOOKREPORT.COM \\nThis book does a good job of explaining the nuts and bolts of how Linux operates. \\nHOSTING RESOLVE \\n\\x0cPreface')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, pathlib, fitz\n",
    "\n",
    "def read_pdf(file_path):\n",
    "  with fitz.open(file_path ) as doc:  # open document\n",
    "      text = chr(12).join([page.get_text() for page in doc])\n",
    "  return {\n",
    "      \"text\": text,\n",
    "      \"file_path\":file_path\n",
    "  }\n",
    "# write as a binary file to support non-ASCII characters\n",
    "# pathlib.Path(fname + \".txt\").write_bytes(text.encode())\n",
    "file_path = \"/content/drive/MyDrive/How Linux Works_ What Every Superuser Should Know ( PDFDrive ).pdf\"\n",
    "res = read_pdf(file_path)\n",
    "len(res[\"text\"]), res[\"text\"][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwgeQ6wKu5CC",
    "outputId": "4a79831b-b1e2-4cea-d9fa-b63b982a17c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(PosixPath('/content/drive/MyDrive/Stochastic_LLMs_do_not_Understand_Langua.pdf'),\n",
       " PosixPath('/content/drive/MyDrive/Khaled_Adrani_Resume_english.pdf'),\n",
       " PosixPath('/content/drive/MyDrive/Root/Archive_2020_2021/emploi.pdf'),\n",
       " PosixPath('/content/drive/MyDrive/Root/Archive_2020_2021/Lettre de motivation Inetum.pdf'),\n",
       " PosixPath('/content/drive/MyDrive/Root/Archive_2020_2021/khaled_adrani.pdf'),\n",
       " PosixPath('/content/drive/MyDrive/Root/Archive_2020_2021/khaled resume demo.pdf'),\n",
       " PosixPath('/content/drive/MyDrive/Root/Archive_2020_2021/PFE_Talan_2019___Oumayma_M__Version_4057_.pdf'),\n",
       " PosixPath('/content/drive/MyDrive/Root/Archive_2020_2021/PFE_Badri__Copy_.pdf'),\n",
       " PosixPath('/content/drive/MyDrive/Root/Archive_2020_2021/khaled_adrani_resume_pfe_old.pdf'),\n",
       " PosixPath('/content/drive/MyDrive/Root/Archive_2020_2021/Fake_News_Detector_PFE/pfe_demo/demo/Khaled_PFE.pdf'))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files = tuple(Path(main_path).rglob('*.pdf'))\n",
    "all_files[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "I4HCexPq6v1w",
    "outputId": "8bed9585-4061-4356-e816-dc7d1e632ac5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'KHALED_ADRANI_RESUME (1)'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files[0].stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGV0eywVuruw",
    "outputId": "44fa9f18-de97-4768-d065-eb04cd25ad3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(841041,\n",
       " '\\x0c \\nHow Linux Works: What Every Superuser Should Know \\nBrian Ward \\nPublished by No Starch Press \\n\\x0cPraise for the First Edition of How Linux Works \\nA great resource. In roughly 350 pages, the book covers all the basics. \\nEWEEK \\nI would definitely recommend this book to those who are interested in Linux, but have not had the experience \\nto know the inner workings of the OS. \\nOREILLYNET \\nOne of the best basic books on learning Linux, written with the power user in mind. Five stars. \\nOPENSOURCE-BOOK-REVIEWS.COM \\nSucceeds admirably because of the way in which its organized and the level of technical detail it offers. \\nKICKSTART NEWS \\nThis is a very different introduction to Linux. Its unflashy, concentrates on the command line, and digs \\naround in the internals rather than on GUI frontends that take the place of more familiar MS Windows tools. \\nTECHBOOKREPORT.COM \\nThis book does a good job of explaining the nuts and bolts of how Linux operates. \\nHOSTING RESOLVE \\n\\x0cPreface')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = read_pdf('/content/drive/MyDrive/The Mistborn Trilogy (The Final Empire; Well of Ascension; Hero of Ages) ( PDFDrive ).pdf')\n",
    "len(res[\"text\"]), res[\"text\"][:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRYZ64sM6fwg"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def post_process_single_record(record, dest, min_length=None):\n",
    "\n",
    "  if min_length and len(record[\"text\"]) < min_length:\n",
    "    print(record[\"file_path\"], \"too short for length \", min_length)\n",
    "\n",
    "  path = Path(record['file_path'])\n",
    "\n",
    "  final_file_name = path.stem\n",
    "\n",
    "  with open(Path(dest) / str(final_file_name+\".txt\"), \"w\" ) as f:\n",
    "    f.write(record[\"text\"])\n",
    "\n",
    "\n",
    "dest = \"/content/drive/MyDrive/data\"\n",
    "\n",
    "batch_callable = partial(post_process_single_record, dest=dest, min_length=2000)\n",
    "batch_callable(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LQMzU8WeyGn0",
    "outputId": "8b42f184-b6c8-45bd-d366-5cf343267485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Noor-Book.com       6 .pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_2021/Lettre de motivation Inetum.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Noor-Book.com       5 .pdf too short for length  2000\n",
      "/content/drive/MyDrive/Noor-Book.com       2 .pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_2021/khaled_adrani.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Noor-Book.com       3 .pdf too short for length  2000\n",
      "/content/drive/MyDrive/Noor-Book.com       4 .pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_2021/khaled resume demo.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/Associative Life/enicar press/JEIS-4.0-PRESS-EDITION-1.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/Associative Life/enicar press/How to write articles with ENICAR press.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/Ressources/internships&work/resources 2019 and 2020/old cv 2019.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/Ressources/internships&work/resources 2019 and 2020/khaled adrani cv.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/Ressources/internships&work/internship exemples/datavision-1.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/Ressources/internships&work/My essays/khaledadrani-cv.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/Ressources/internships&work/My essays/Khaled Adrani CV Eng.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/Ressources/internships&work/INSA Toulouse/Exemple-vierge-de-CV-ingenieur-junior.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/Ressources/pfe 2020/Lettre de motivation Inetum.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/Ressources/pfe 2020/lettre de motivation khaled adrani.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/Travaris_Custom/Clustering.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/static/OST_2020.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/static/khaled resume freelance.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Root/Archive_2020_and_before/static/AdraniKhaled-NDG Linux Essent-certificate.pdf too short for length  2000\n",
      "/content/drive/MyDrive/Public/GDSpdf/GLDS383 - Long-read sequencing reveals increased occurrence of genomic variants and adenosine methylation in Bacillus pumilus SAFR-032.pdf too short for length  2000\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "from typing import Callable\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def execute_tasks(task:Callable,\n",
    "                       inputs,\n",
    "                       executor_entity,\n",
    "                       post_process_global:Callable = None,\n",
    "                       post_process_batch:Callable = None,\n",
    "                       max_workers=10, *args, **kwargs):\n",
    "  data = []\n",
    "  try:\n",
    "    with executor_entity(max_workers=max_workers) as executor:\n",
    "        # Start the load operations and mark each future with its URL\n",
    "        future_to_result = {executor.submit(task, input, *args, **kwargs): input for input in inputs}\n",
    "        #print(future_to_url)\n",
    "        for future in concurrent.futures.as_completed(future_to_result):\n",
    "            #print(future)\n",
    "            url = future_to_result[future]\n",
    "            try:\n",
    "                record = future.result()\n",
    "\n",
    "                if post_process_batch:\n",
    "                  record = post_process_batch(record)\n",
    "\n",
    "                data.append(record)\n",
    "            except Exception as exc:\n",
    "                print('a certain context generated an exception: ',exc)\n",
    "\n",
    "    if post_process_global:\n",
    "      data = post_process_global(data)\n",
    "\n",
    "    return data\n",
    "  except Exception as err:\n",
    "    print('Global Error!: ', str(err))\n",
    "    raise err\n",
    "\n",
    "\n",
    "res = execute_tasks(task=read_pdf, inputs=all_files,\n",
    "                    executor_entity=concurrent.futures.ThreadPoolExecutor,\n",
    "                    post_process_batch=batch_callable\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVIryUae-X6E"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQkBVbkPsrb8"
   },
   "source": [
    "## online datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "W7_LgsbgMhe8"
   },
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "def execute_task_batch(task,inputs, post_processing=None, max_workers=10, *args, **kwargs):\n",
    "  data = []\n",
    "  try:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Start the load operations and mark each future with its URL\n",
    "        future_to_result = {executor.submit(task, input, *args, **kwargs): input for input in inputs}\n",
    "        #print(future_to_url)\n",
    "        for future in concurrent.futures.as_completed(future_to_result):\n",
    "            #print(future)\n",
    "            url = future_to_result[future]\n",
    "            try:\n",
    "                record = future.result()\n",
    "\n",
    "                data.append(record)\n",
    "            except Exception as exc:\n",
    "                print('generated an exception: ',exc)\n",
    "\n",
    "    if post_processing:\n",
    "      data = post_processing(data)\n",
    "\n",
    "    return data\n",
    "  except Exception as err:\n",
    "    print('Error in concurrent_task: ', str(err))\n",
    "    raise err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJ-88jfhNCH6"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def download_from_hugging_face(offset=0, dataset_name=\"nvidia/HelpSteer\"):\n",
    "  url = \"https://datasets-server.huggingface.co/rows\"\n",
    "\n",
    "  params = { #parametrized\n",
    "      \"dataset\": dataset_name,\n",
    "      \"config\": \"default\",\n",
    "      \"split\": \"train\",\n",
    "      \"offset\": str(offset),\n",
    "      \"length\": \"100\"\n",
    "  }\n",
    "\n",
    "  response = requests.get(url, params=params)\n",
    "\n",
    "  if response.status_code == 200:\n",
    "      return response.json()\n",
    "      # print(help_steer_data)\n",
    "  else:\n",
    "    print(response.content)\n",
    "    print(\"Error:\", response.status_code)\n",
    "    raise ValueError(response.status_code)\n",
    "\n",
    "  time.sleep(0.3)\n",
    "\n",
    "def post_process_hugging_face(result):\n",
    "  dataset = []\n",
    "\n",
    "  for l in result:\n",
    "    dataset.extend([row['row']['prompt'] + \"<sep>\" + row['row']['response'] for row in l['rows']])\n",
    "\n",
    "  return dataset\n",
    "\n",
    "\n",
    "result = execute_task_batch(task=download_from_hugging_face,\n",
    "                            post_processing=post_process_hugging_face,\n",
    "                            inputs=range(10000))\n",
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mv2lcYEJfV7u"
   },
   "outputs": [],
   "source": [
    "total_result = \"\\n\".join(result)\n",
    "\n",
    "with open(\"/content/drive/MyDrive/data/nvidia_help_steer_10k.txt\",\"w\") as f:\n",
    "  f.write(total_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFe8b-crexqm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HliYJD1Xhyyj"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://datasets-server.huggingface.co/rows\"\n",
    "\n",
    "ls = []\n",
    "\n",
    "for offset in range(10000):\n",
    "  params = {\n",
    "      \"dataset\": \"HuggingFaceH4/no_robots\",\n",
    "      \"config\": \"default\",\n",
    "      \"split\": \"train\",\n",
    "      \"offset\": str(offset),\n",
    "      \"length\": \"100\"\n",
    "  }\n",
    "\n",
    "  response = requests.get(url, params=params)\n",
    "\n",
    "  if response.status_code == 200:\n",
    "      ls.append(response.json())\n",
    "      # print(help_steer_data)\n",
    "  else:\n",
    "    print(response.content)\n",
    "    print(\"Error:\", response.status_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "id": "UPIrfpVrja9a",
    "outputId": "1c9a1790-401b-4bea-f717-ae2c9115de25"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkerts portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species compositionwhich was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. We cant ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there, says Rinkert.<sep>Please summarize the goals for scientists in this text:\\n\\nWithin three days, the intertwined cup nest of grasses was complete, featuring a canopy of overhanging grasses to conceal it. And decades later, it served as Rinkerts portal to the past inside the California Academy of Sciences. Information gleaned from such nests, woven long ago from species in plant communities called transitional habitat, could help restore the shoreline in the future. Transitional habitat has nearly disappeared from the San Francisco Bay, and scientists need a clearer picture of its original species compositionwhich was never properly documented. With that insight, conservation research groups like the San Francisco Bay Bird Observatory can help guide best practices when restoring the native habitat that has long served as critical refuge for imperiled birds and animals as adjacent marshes flood more with rising sea levels. We cant ask restoration ecologists to plant nonnative species or to just take their best guess and throw things out there, says Rinkert.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2 = []\n",
    "\n",
    "for l in ls:\n",
    "  result_2.extend([row['row']['prompt'] + \"<sep>\" + row['row']['messages'][0]['content'] for row in l['rows']])\n",
    "\n",
    "result_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_Fm8VjIj6kE",
    "outputId": "66977b9b-1d6b-463c-ed43-99fe1f55d5fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YbZwPwiOjGLT",
    "outputId": "4da91690-1ac5-4d1b-b8b8-b93e2f81b4d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-03 11:22:36--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: input.txt.2\n",
      "\n",
      "\r",
      "input.txt.2           0%[                    ]       0  --.-KB/s               \r",
      "input.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.009s  \n",
      "\n",
      "2024-05-03 11:22:37 (116 MB/s) - input.txt.2 saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    tiny = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hF0JKGXUlFpb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4KbybIU-UOh"
   },
   "source": [
    "## syunthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "AtBDd3nx-Vsh",
    "outputId": "16f9aec9-4abf-4694-9b86-41a60ceaf60c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tiny' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5bd4b4bc64f3>\u001b[0m in \u001b[0;36m<cell line: 220>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;31m# Combine the original sentences with the additional sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m \u001b[0mall_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madditional_sentences\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mqs1\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mqs2\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mqs3\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mqs4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtiny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;31m# all_sentences =  [ \"<sos>\" + sent + \"<eos>\" for sent in all_sentences]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tiny' is not defined"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Natural Language Processing (NLP) is a field of artificial intelligence.\",\n",
    "    \"NLP focuses on the interaction between computers and humans through natural language.\",\n",
    "    \"NLP enables computers to understand, interpret, and generate human language.\",\n",
    "    \"NLP techniques are used in various applications such as sentiment analysis, machine translation, and text summarization.\",\n",
    "    \"Tokenization is the process of breaking text into smaller units such as words or subwords.\",\n",
    "    \"Part-of-speech tagging assigns grammatical information to words in a sentence.\",\n",
    "    \"Named entity recognition identifies and classifies named entities in text into predefined categories.\",\n",
    "    \"Text classification categorizes text into predefined classes or categories.\",\n",
    "    \"Sentiment analysis determines the sentiment expressed in a piece of text, such as positive, negative, or neutral.\",\n",
    "    \"Machine translation translates text from one language to another using NLP techniques.\",\n",
    "    \"Text summarization generates concise summaries of longer texts, preserving key information.\",\n",
    "    \"Word embeddings represent words as dense vectors in a continuous vector space.\",\n",
    "    \"Recurrent Neural Networks (RNNs) are a type of neural network architecture commonly used in NLP tasks.\",\n",
    "    \"Long Short-Term Memory (LSTM) networks are a type of RNN architecture designed to capture long-term dependencies in sequences.\",\n",
    "    \"Attention mechanisms allow models to focus on relevant parts of the input sequence in NLP tasks.\",\n",
    "    \"Transformer models, such as BERT and GPT, have achieved state-of-the-art performance in various NLP tasks.\",\n",
    "    \"Named after Alan Turing, the Turing Test assesses a machine's ability to exhibit intelligent behavior indistinguishable from that of a human.\",\n",
    "    \"Semantic analysis focuses on understanding the meaning of text beyond its literal interpretation.\",\n",
    "    \"Syntactic analysis, or parsing, analyzes the grammatical structure of sentences.\",\n",
    "    \"Lexical analysis involves identifying and analyzing the vocabulary and word structure of a language.\"\n",
    "]\n",
    "\n",
    "additional_sentences = [\n",
    "    \"Deep learning techniques have revolutionized the field of Natural Language Processing (NLP).\",\n",
    "    \"NLP algorithms can extract insights and patterns from large volumes of text data.\",\n",
    "    \"Word frequency analysis is a basic technique used in NLP for understanding document content.\",\n",
    "    \"Dependency parsing identifies the grammatical relationships between words in a sentence.\",\n",
    "    \"Named entity recognition can identify entities such as people, organizations, and locations in text.\",\n",
    "    \"Topic modeling algorithms, like Latent Dirichlet Allocation (LDA), uncover hidden themes in text collections.\",\n",
    "    \"Word sense disambiguation resolves the meaning of ambiguous words based on context.\",\n",
    "    \"N-gram models are simple yet powerful statistical models used for language modeling.\",\n",
    "    \"Conversational agents, or chatbots, use NLP techniques to engage in human-like conversations.\",\n",
    "    \"Sentiment lexicons provide lists of words annotated with sentiment polarity for sentiment analysis tasks.\",\n",
    "    \"Text normalization standardizes text data by converting it to a common format, such as lowercase.\",\n",
    "    \"Named entity linking connects named entities mentioned in text to corresponding entries in knowledge bases.\",\n",
    "    \"Information retrieval systems use NLP to retrieve relevant documents in response to user queries.\",\n",
    "    \"Sequence-to-sequence models are used for tasks like machine translation and text summarization.\",\n",
    "    \"Knowledge graphs organize structured information to represent relationships between entities.\",\n",
    "    \"Natural Language Understanding (NLU) focuses on comprehending the meaning and intent behind text.\",\n",
    "    \"Named entity recognition can be challenging for languages with complex morphologies.\",\n",
    "    \"Automatic speech recognition systems transcribe spoken language into text using NLP techniques.\",\n",
    "    \"Syntax trees represent the hierarchical structure of sentences in syntactic analysis.\",\n",
    "    \"Semantic role labeling identifies the roles played by words in a sentence, such as agent or patient.\"\n",
    "]\n",
    "\n",
    "\n",
    "qs1  = [\n",
    "    \"Hi, how are you today?<sep>I'm doing well, thank you!\",\n",
    "    \"Hello, what's up?<sep>Not much, just relaxing.\",\n",
    "    \"Good morning! How did you sleep?<sep>I slept like a baby, thanks for asking.\",\n",
    "    \"Hey there, how's your day going?<sep>It's going great, thanks!\",\n",
    "    \"Hi, it's nice to see you!<sep>Nice to see you too!\",\n",
    "    \"Hello, how have you been?<sep>I've been good, keeping busy.\",\n",
    "    \"Good afternoon! How's work going?<sep>Work's going fine, nothing special.\",\n",
    "    \"Hey, long time no see! What have you been up to?<sep>Not much, just the usual stuff.\",\n",
    "    \"Hi, how's the weather today?<sep>The weather's nice, sunny and warm.\",\n",
    "    \"Hello, any plans for the weekend?<sep>Not yet, just taking it easy.\",\n",
    "    \"Good evening! How was your day?<sep>It was pretty good, thanks for asking.\",\n",
    "    \"Hey, how's the family doing?<sep>The family's doing well, thanks for asking.\",\n",
    "    \"Hi there, what's new with you?<sep>Not much, just chilling.\",\n",
    "    \"Hello, did you watch any good movies lately?<sep>Yes, I watched a great movie last night.\",\n",
    "    \"Good morning! Did you have breakfast?<sep>Yes, I had a delicious breakfast this morning.\",\n",
    "    \"Hey, how's your pet doing?<sep>My pet's doing great, thanks for asking.\",\n",
    "    \"Hi, any exciting plans for the holidays?<sep>Yes, I'm planning a trip with my friends.\",\n",
    "    \"Hello, how was your weekend?<sep>It was fantastic, I had a lot of fun.\",\n",
    "    \"Good afternoon! Have you tried the new restaurant in town?<sep>Yes, I tried it last week, it was amazing.\",\n",
    "    \"Hey, did you hear about the new job opening?<sep>Yes, I heard about it, I might apply.\",\n",
    "]\n",
    "\n",
    "qs2  = [\n",
    "    \"Hey, how's it going?<sep>Not too bad, thanks for asking!\",\n",
    "    \"Hi, what have you been up to lately?<sep>Just working on some projects.\",\n",
    "    \"Hello, how was your weekend?<sep>It was relaxing, I spent time with family.\",\n",
    "    \"Good morning! Did you sleep well?<sep>Yes, I had a great night's sleep.\",\n",
    "    \"Hey there, any plans for the evening?<sep>Just going to watch a movie.\",\n",
    "    \"Hi, did you see the news today?<sep>Yes, it was quite interesting.\",\n",
    "    \"Hello, how's your day been so far?<sep>Busy, but productive.\",\n",
    "    \"Good afternoon! How's the weather outside?<sep>It's sunny and warm.\",\n",
    "    \"Hey, what are you reading these days?<sep>I'm reading a new novel.\",\n",
    "    \"Hi, have you tried that new coffee shop?<sep>Not yet, but I plan to.\",\n",
    "    \"Hello, any exciting updates in your life?<sep>Not much, same old routine.\",\n",
    "    \"Good evening! How was your day at work?<sep>It was challenging, but rewarding.\",\n",
    "    \"Hey, did you catch the game last night?<sep>Yes, it was intense.\",\n",
    "    \"Hi, how's your family doing?<sep>They're doing well, thanks for asking.\",\n",
    "    \"Hello, any plans for the holidays?<sep>Yes, I'm traveling to visit relatives.\",\n",
    "    \"Good morning! Have you had breakfast yet?<sep>Yes, I had a healthy breakfast.\",\n",
    "    \"Hey there, do you enjoy cooking?<sep>Yes, it's one of my hobbies.\",\n",
    "    \"Hi, did you hear about the new movie release?<sep>Yes, I'm excited to watch it.\",\n",
    "    \"Hello, how's your pet doing?<sep>She's doing great, full of energy.\",\n",
    "    \"Good afternoon! How's the new project going?<sep>It's progressing well, thank you.\",\n",
    "    \"Hey, what's your favorite hobby?<sep>I enjoy playing the guitar.\",\n",
    "    \"Hi, any recommendations for a good book?<sep>Yes, I can suggest a few.\",\n",
    "    \"Hello, what's your favorite season?<sep>I love the fall season.\",\n",
    "    \"Good evening! Any plans for the weekend?<sep>I'm going hiking with friends.\",\n",
    "    \"Hey, have you traveled anywhere interesting lately?<sep>Yes, I went on a road trip.\",\n",
    "    \"Hi, what's your favorite movie genre?<sep>I enjoy watching comedies.\",\n",
    "    \"Hello, any tips for staying productive?<sep>Stay organized and prioritize tasks.\",\n",
    "    \"Good morning! How's your morning routine?<sep>It's going smoothly, thank you.\",\n",
    "    \"Hey there, do you enjoy outdoor activities?<sep>Yes, I love hiking and camping.\",\n",
    "    \"Hi, any new restaurants you've tried recently?<sep>Yes, I tried a new sushi place.\",\n",
    "    \"Hello, what's your favorite cuisine?<sep>I enjoy Italian food the most.\",\n",
    "    \"Good afternoon! How do you like to unwind?<sep>I like to read or listen to music.\",\n",
    "    \"Hey, do you follow any sports?<sep>Yes, I'm a fan of soccer.\",\n",
    "    \"Hi, any plans for the upcoming holiday?<sep>Not yet, but I'm thinking of traveling.\",\n",
    "    \"Hello, what's your favorite type of music?<sep>I enjoy listening to jazz.\",\n",
    "    \"Good evening! Do you like attending concerts?<sep>Yes, it's always a great experience.\",\n",
    "    \"Hey, any interesting podcasts you're listening to?<sep>Yes, I have a few favorites.\",\n",
    "    \"Hi, how do you stay motivated?<sep>Setting goals and staying focused.\",\n",
    "    \"Hello, any new skills you're learning?<sep>Yes, I'm learning to cook new recipes.\",\n",
    "]\n",
    "\n",
    "qs3 = [\n",
    "    \"Hey, how's the weather today?<sep>It's a bit cloudy, but not too bad.\",\n",
    "    \"Hi, have you tried the new restaurant downtown?<sep>Yes, I went there last week, it was delicious.\",\n",
    "    \"Hello, what's your favorite type of cuisine?<sep>I really enjoy Mexican food.\",\n",
    "    \"Good morning! Do you have any plans for the weekend?<sep>Yes, I'm going to visit my family.\",\n",
    "    \"Hey there, have you ever traveled abroad?<sep>Yes, I've been to Europe a couple of times.\",\n",
    "    \"Hi, do you like to cook?<sep>Yes, I find it quite relaxing.\",\n",
    "    \"Hello, have you seen any good movies lately?<sep>Not recently, but I'm looking forward to some upcoming releases.\",\n",
    "    \"Good afternoon! What's your favorite outdoor activity?<sep>I love going for hikes in the mountains.\",\n",
    "    \"Hey, how's your day going so far?<sep>It's been pretty good, thanks for asking.\",\n",
    "    \"Hi, any plans for the evening?<sep>I'm just going to relax at home.\",\n",
    "    \"Hello, have you ever been to a music festival?<sep>Yes, they're always a lot of fun.\",\n",
    "    \"Good evening! How was your day at work?<sep>It was busy, but productive.\",\n",
    "    \"Hey, do you enjoy gardening?<sep>Yes, it's a great way to unwind.\",\n",
    "    \"Hi, what's your favorite book?<sep>I have many favorites, but one of them is 'To Kill a Mockingbird'.\",\n",
    "    \"Hello, do you have any pets?<sep>Yes, I have a dog named Max.\",\n",
    "    \"Good morning! Did you sleep well?<sep>Yes, I had a restful night's sleep.\",\n",
    "    \"Hey there, how do you like to spend your weekends?<sep>I enjoy going for walks and exploring new places.\",\n",
    "    \"Hi, have you ever been skydiving?<sep>No, but it's something I'd like to try someday.\",\n",
    "    \"Hello, do you enjoy going to museums?<sep>Yes, I find them very interesting and informative.\",\n",
    "    \"Good afternoon! What's your favorite season?<sep>I love the springtime, everything feels so fresh and vibrant.\",\n",
    "    \"Hey, do you have any siblings?<sep>Yes, I have a younger brother and an older sister.\",\n",
    "    \"Hi, what's your favorite holiday?<sep>I love Christmas, it's such a festive time of year.\",\n",
    "    \"Hello, do you like to go camping?<sep>Yes, I enjoy spending time in nature.\",\n",
    "    \"Good evening! What's your favorite movie genre?<sep>I'm a fan of science fiction films.\",\n",
    "    \"Hey, have you ever been snorkeling?<sep>Yes, it's an amazing experience.\",\n",
    "    \"Hi, what's your favorite dessert?<sep>I have a sweet tooth, so I love all kinds of desserts.\",\n",
    "    \"Hello, do you enjoy going to the beach?<sep>Yes, I find it very relaxing.\",\n",
    "    \"Good morning! What's your favorite way to start the day?<sep>I like to have a cup of coffee and read the news.\",\n",
    "    \"Hey there, have you ever been to a music concert?<sep>Yes, I've been to several concerts, they're always so much fun.\",\n",
    "    \"Hi, what's your favorite type of music?<sep>I enjoy listening to rock and alternative.\",\n",
    "    \"Hello, do you have any favorite TV shows?<sep>Yes, I have a few, but one of them is 'Game of Thrones'.\",\n",
    "    \"Good afternoon! Do you like to go for walks?<sep>Yes, I find it very relaxing and refreshing.\",\n",
    "    \"Hey, what's your favorite type of cuisine?<sep>I really enjoy Italian food.\",\n",
    "    \"Hi, have you ever been to a live theater performance?<sep>Yes, it's always a great experience.\",\n",
    "    \"Hello, do you like to go hiking?<sep>Yes, it's one of my favorite outdoor activities.\",\n",
    "    \"Good evening! What's your favorite sport?<sep>I enjoy playing soccer.\",\n",
    "    \"Hey, have you ever tried rock climbing?<sep>Yes, it's a challenging but rewarding activity.\",\n",
    "    \"Hi, what's your favorite hobby?<sep>I enjoy playing the guitar in my free time.\",\n",
    "    \"Hello, have you ever traveled solo?<sep>Yes, it was an enriching experience.\",\n",
    "]\n",
    "\n",
    "qs4  = [\n",
    "    \"Hey there! How's everything going on your end?<sep>Not too shabby, thanks for asking!\",\n",
    "    \"Hi! I was wondering how your day has been so far?<sep>It's been quite productive, actually.\",\n",
    "    \"Hello! What's the latest update on your side of the world?<sep>Just tackling tasks and staying busy.\",\n",
    "    \"Good morning! Have you had a chance to grab a cup of coffee yet?<sep>Yes, I'm enjoying my morning brew.\",\n",
    "    \"Hey! Do you have any plans for the evening ahead?<sep>Thinking of trying out that new restaurant.\",\n",
    "    \"Hi there! What's been on your mind lately?<sep>Just pondering over some new ideas.\",\n",
    "    \"Hello! Any exciting news to share from your end?<sep>Not much, just taking it one day at a time.\",\n",
    "    \"Good afternoon! How's the weather treating you today?<sep>It's a bit cloudy, but otherwise fine.\",\n",
    "    \"Hey! Have you come across any interesting articles lately?<sep>Yes, I stumbled upon a fascinating read.\",\n",
    "    \"Hi! How's your day unfolding so far?<sep>It's been a mix of work and relaxation.\",\n",
    "    \"Hello! Have you explored any new hobbies recently?<sep>Yes, I've been dabbling in photography.\",\n",
    "    \"Good evening! Any plans for the night ahead?<sep>Just catching up on some reading.\",\n",
    "    \"Hey there! Did you catch the latest episode of your favorite show?<sep>Yes, it was quite entertaining.\",\n",
    "    \"Hi! How's your family doing these days?<sep>They're all doing well, thank you.\",\n",
    "    \"Hello! Any travel plans on the horizon?<sep>Yes, I'm considering a weekend getaway.\",\n",
    "    \"Good morning! Did you have any interesting dreams last night?<sep>Yes, I had a vivid dream about flying.\",\n",
    "    \"Hey! Have you ever tried your hand at painting?<sep>Yes, it's a relaxing hobby.\",\n",
    "    \"Hi there! What's your take on the latest political developments?<sep>It's quite a contentious issue.\",\n",
    "    \"Hello! Any book recommendations you'd like to share?<sep>Yes, I have a few in mind.\",\n",
    "    \"Good afternoon! How's your energy level today?<sep>Feeling pretty good, thanks for asking.\",\n",
    "    \"Hey! What's your favorite way to spend a lazy Sunday?<sep>Curling up with a good book.\",\n",
    "    \"Hi! How do you unwind after a long day?<sep>I like to take a leisurely stroll.\",\n",
    "    \"Hello! What's your favorite thing about the changing seasons?<sep>I enjoy the colors of fall.\",\n",
    "    \"Good evening! Any plans for the upcoming holiday season?<sep>Yes, I'm looking forward to it.\",\n",
    "    \"Hey! Have you ever been on a spontaneous road trip?<sep>Yes, it was quite an adventure.\",\n",
    "    \"Hi! What's your go-to comfort food?<sep>Nothing beats a bowl of hot soup.\",\n",
    "    \"Hello! How do you stay inspired during challenging times?<sep>By focusing on the positive.\",\n",
    "    \"Good morning! Any podcasts you've been listening to lately?<sep>Yes, I've discovered some new ones.\",\n",
    "    \"Hey there! Do you believe in the power of positive thinking?<sep>Absolutely, it's transformative.\",\n",
    "    \"Hi! What's your favorite memory from childhood?<sep>Playing in the backyard with friends.\",\n",
    "    \"Hello! Any advice you'd give to your younger self?<sep>To cherish every moment.\",\n",
    "    \"Good afternoon! What's your favorite type of cuisine to cook?<sep>I love experimenting with flavors.\",\n",
    "    \"Hey! How do you balance work and personal life?<sep>By setting boundaries and priorities.\",\n",
    "    \"Hi! Any new languages you're interested in learning?<sep>Yes, I'm learning Spanish.\",\n",
    "    \"Hello! What's your favorite outdoor activity?<sep>Hiking in the mountains.\",\n",
    "    \"Good evening! Do you have any secret talents?<sep>Not really, just hobbies.\",\n",
    "    \"Hey! How do you overcome writer's block?<sep>By taking a break and coming back refreshed.\",\n",
    "    \"Hi! Any music genres you're exploring lately?<sep>Yes, I'm into indie music.\",\n",
    "    \"Hello! How do you handle stress?<sep>By practicing mindfulness and deep breathing.\",\n",
    "    \"Good morning! What's your morning ritual?<sep>A cup of tea and some light stretching.\",\n",
    "    \"Hey there! Do you have any guilty pleasures?<sep>Watching reality TV shows.\",\n",
    "    \"Hi! What's your favorite thing about the holiday season?<sep>The festive atmosphere.\",\n",
    "    \"Hello! Any travel destinations on your bucket list?<sep>Visiting Japan is at the top.\",\n",
    "    \"Good afternoon! How do you celebrate personal victories?<sep>By treating myself to something nice.\",\n",
    "    \"Hey! What's your favorite childhood movie?<sep>The Lion King.\",\n",
    "    \"Hi! Do you prefer mornings or evenings?<sep>Evenings, for sure.\",\n",
    "    \"Hello! What's your favorite way to exercise?<sep>Going for a run in the park.\",\n",
    "    \"Good evening! How do you recharge after a busy day?<sep>By spending time with loved ones.\",\n",
    "    \"Hey! What's your favorite time of day?<sep>The golden hour, just before sunset.\",\n",
    "    \"Hi! Do you have any morning rituals?<sep>Meditation and journaling.\",\n",
    "    \"Hello! What's your favorite thing about nature?<sep>The tranquility it brings.\",\n",
    "    \"Good morning! Any plans for the weekend?<sep>Just relaxing and unwinding.\",\n",
    "    \"Hey there! How do you find inspiration?<sep>By observing the world around me.\",\n",
    "    \"Hi! What's your favorite thing about your hometown?<sep>The sense of community.\",\n",
    "    \"Hello! How do you stay focused?<sep>By setting clear goals and priorities.\",\n",
    "    \"Good afternoon! Any interesting books you've read recently?<sep>Yes, a thought-provoking novel.\",\n",
    "    \"Hey! What's your favorite type of art?<sep>Abstract paintings.\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Combine the original sentences with the additional sentences\n",
    "all_sentences = sentences + additional_sentences + qs1 +qs2 +qs3 +qs4 + [tiny]\n",
    "\n",
    "# all_sentences =  [ \"<sos>\" + sent + \"<eos>\" for sent in all_sentences]\n",
    "\n",
    "corpus = \"\".join(all_sentences)\n",
    "corpus[:100], len(all_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_piC_gv2lGIQ"
   },
   "source": [
    "## Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ih90TUZCMmzB",
    "outputId": "f8494f9b-db41-40e7-8f88-0d5f518c978a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "random.seed(42)\n",
    "\n",
    "class DatasetLoader:\n",
    "  def __init__(self,path=\"/content/drive/MyDrive/data\"):\n",
    "    self.path = path\n",
    "    self.file_list = list(Path(self.path).glob(\"*.txt\"))\n",
    "    self.index = 0\n",
    "\n",
    "\n",
    "\n",
    "  def __iter__(self):\n",
    "    return self\n",
    "\n",
    "  def __next__(self):\n",
    "    try:\n",
    "      with open(self.file_list[self.index], \"r\") as f:\n",
    "        self.index += 1\n",
    "        return f.read()\n",
    "    except IndexError as error:\n",
    "        raise StopIteration from error\n",
    "\n",
    "\n",
    "class DataSplitter:\n",
    "  def __init__(self, data_loader:DatasetLoader):\n",
    "    self.data_loader = data_loader\n",
    "\n",
    "    self.n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "    self.train_data = self.file_list[:n]\n",
    "    self.val_data = self.file_list[n:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for doc in DatasetLoader():\n",
    "  print(doc[:10])\n",
    "  break\n",
    "\n",
    "dl = DatasetLoader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RfS4BPYJ_lyd"
   },
   "source": [
    "## simple tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8vHpAtuWvgTu",
    "outputId": "9b5935c5-30df-4179-c751-27e531539283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20000\n",
      "[165, 70]\n",
      "before there\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "dataset_loader = DatasetLoader()\n",
    "\n",
    "def remove_duplicates_preserve_order(input_list):\n",
    "    seen = set()\n",
    "    return [x for x in input_list if not (x in seen or seen.add(x))]\n",
    "\n",
    "special_tokens = ['<sos>', '<eos>','<unk>','<pad>','<mask>', '<sep>']\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Define a function to yield tokenized sentences from the iterator\n",
    "def yield_tokens(iterator):\n",
    "    for text in iterator:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 20000\n",
    "\n",
    "# Build vocabulary from the tokenized sentences\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset_loader),\n",
    "                                   max_tokens = vocab_size,\n",
    "                                  specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "stoi = vocab.get_stoi()\n",
    "itos = vocab.get_itos()\n",
    "encode = lambda text: [ stoi.get(token, stoi[\"<unk>\"]) for token in tokenizer(text) ]\n",
    "decode = lambda indexes: \" \".join([ itos[index] for index in indexes])\n",
    "encoded = encode(\"before there\")\n",
    "print(encoded), print(decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B9U5pSDv_zxu",
    "outputId": "177a56c9-4670-4996-9455-f27764dd2ee7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17208819"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DatasetLoader()\n",
    "text = \"\"\n",
    "for _ in range(130):\n",
    "  try:\n",
    "    text += next(dl)\n",
    "  except:\n",
    "    continue\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4stRgvnYARW_",
    "outputId": "0f17bcdb-0163-4f00-b0a6-f31e89d9dbf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3386152, tensor([1650,  522,  632, 1137, 1658, 1835,  954,    7, 1137,    7]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "len(data), data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qSdXCbAAYqb",
    "outputId": "6124b109-c93b-4013-ca8d-2b9b6c546d90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64]),\n",
       " tensor([ 505129,  126202,  254302,  476182, 2670198, 3245885, 1724076, 2006780,\n",
       "         1660867, 2754601, 1283785, 1619270, 3124404,  268983, 1000195, 2816570,\n",
       "         1193085,  752402, 2033676, 2250682, 2919172, 3068063, 2470629, 2087603,\n",
       "         2670158, 1560142, 1544936, 2037864, 1332203, 1888805,   10546, 3051819,\n",
       "         2771001, 2238100, 3128562, 2412648,  695591, 2630988, 3084089, 1702127,\n",
       "          836152,  385919, 2807821,  172767, 2687372, 1583526, 2824002, 1479178,\n",
       "         2362570, 2991044, 2533051, 1608231,  364555, 3218570, 2773549,  367142,\n",
       "          479876, 3241927, 2500183,   68755, 2063237, 1728715, 2899410,  150934]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "ix.shape, ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "078gRBhVBaRN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUJ9EQ_wAtZo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3oAGsLAWAsT_",
    "outputId": "1ae1c9f8-b6f2-4ad1-b6b2-b30451bb44cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "ENDLCBMUBa88",
    "outputId": "fe13c2c6-7103-4e43-f544-d947deeb8c9a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'people want us to respect their feelings , they should respect ours .  on 7 january 2015 <unk> <unk> <unk> several staff members of the french magazine <unk> <unk> , because'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LHkYdG71C2PT"
   },
   "outputs": [],
   "source": [
    "class SubsequenceIterator:\n",
    "    def __init__(self, data, block_size, batch_size):\n",
    "        self.data = data\n",
    "        self.data_length = len(data)\n",
    "        self.block_size = block_size\n",
    "        self.batch_size = batch_size\n",
    "        self.index = 0  # Start index\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index >= self.data_length - self.block_size:\n",
    "            # If we have reached the end of the data, raise StopIteration\n",
    "            raise StopIteration\n",
    "\n",
    "        # Initialize lists to store x and y subsequences for the batch\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        # Generate the batch of indices\n",
    "        for _ in range(self.batch_size):\n",
    "            # Extract subsequence x from index to index + block_size\n",
    "            x_subsequence = self.data[self.index:self.index+self.block_size]\n",
    "            # Extract subsequence y from index + 1 to index + block_size + 1\n",
    "            y_subsequence = self.data[self.index+1:self.index+self.block_size+1]\n",
    "\n",
    "            # Append the subsequences to the batch lists\n",
    "            x_batch.append(x_subsequence)\n",
    "            y_batch.append(y_subsequence)\n",
    "\n",
    "            # Move to the next index\n",
    "            self.index += 1\n",
    "\n",
    "            if self.index >= self.data_length - self.block_size:\n",
    "                # If we reach the end of the data, break the loop\n",
    "                break\n",
    "\n",
    "        # Convert the lists of subsequences to tensors and return\n",
    "        return torch.stack(x_batch), torch.stack(y_batch)\n",
    "\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "iterator = SubsequenceIterator(data, block_size, batch_size)\n",
    "\n",
    "# Iterate over the batches\n",
    "for x_batch, y_batch in iterator:\n",
    "    # Do something with the batches\n",
    "    print(x_batch.shape, y_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uXW9s1A__pFv"
   },
   "outputs": [],
   "source": [
    "# how to handle this as a stream, not all at once\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfLOOdVV_lKB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KM6u7RGz_qkS"
   },
   "source": [
    "## try all tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCcc4ARptzso"
   },
   "outputs": [],
   "source": [
    "def pad_sequence(seq, max_length, pad_token):\n",
    "    pad_length = max_length - len(seq)\n",
    "    padding = torch.full((pad_length,), pad_token, dtype=seq.dtype)\n",
    "    padded_seq = torch.cat((seq, padding))\n",
    "    return padded_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dKxlGivvSM2",
    "outputId": "2ca795c5-24ed-46dc-c45b-8e0a097f51a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stoi[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vkYNHt2TtvAX",
    "outputId": "9f3c1809-db42-4602-a3fd-55b980b03ab8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165439"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_batches(data, block_size, batch_size):\n",
    "    data = torch.tensor(encode(data), dtype=torch.long)\n",
    "    batches_x = []\n",
    "    batches_y = []\n",
    "\n",
    "    for i in range(len(data) - block_size + 1):\n",
    "        x_batch = data[i:i+block_size]\n",
    "        y_batch = data[i+1:i+block_size+1]\n",
    "        batches_x.append(x_batch)\n",
    "        batches_y.append(y_batch)\n",
    "\n",
    "    max_length = max(len(seq) for seq in batches_x)\n",
    "\n",
    "    pad_token = stoi[\"<pad>\"]\n",
    "\n",
    "    for x_seq, y_seq in zip(batches_x,batches_y ):\n",
    "      x_padded = pad_sequence(x_seq, max_length, pad_token)\n",
    "      y_padded = pad_sequence(y_seq, max_length, pad_token)\n",
    "\n",
    "      yield x_padded.to(device), y_padded.to(device)\n",
    "\n",
    "    # # Pad sequences to ensure they all have the same length\n",
    "    # batches_x_padded = [pad_sequence(seq, max_length, pad_token) for seq in batches_x]\n",
    "    # batches_y_padded = [pad_sequence(seq, max_length, pad_token) for seq in batches_y]\n",
    "\n",
    "    # return batches_x_padded, batches_y_padded\n",
    "\n",
    "\n",
    "ls  = tuple(get_all_batches(data, 32, 64))\n",
    "len(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jT23XQUFwNyb",
    "outputId": "05211f6a-8913-4c89-dbb6-e20ceee028c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  78,  500,  829,   54,  230, 6552,  130,  112, 8169, 6989,  970,   36,\n",
       "           86, 7660,  470,    0,   16,    6,  144,  957,    7,   78,  500,  829,\n",
       "         1732,  354, 2386,    4,   11, 4364, 5400,  567]),\n",
       " tensor([ 500,  829,   54,  230, 6552,  130,  112, 8169, 6989,  970,   36,   86,\n",
       "         7660,  470,    0,   16,    6,  144,  957,    7,   78,  500,  829, 1732,\n",
       "          354, 2386,    4,   11, 4364, 5400,  567,    5]))"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-l-Ij_GDPdL-"
   },
   "outputs": [],
   "source": [
    "def divide_into_batches(data, batch_size):\n",
    "    num_batches = (len(data) + batch_size - 1) // batch_size\n",
    "    batches = [data[i*batch_size:(i+1)*batch_size] for i in range(num_batches)]\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ltkWE3lyzgWE",
    "outputId": "1aaf6c97-7fdc-4deb-dc91-1e7157605d42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches: 2585\n"
     ]
    }
   ],
   "source": [
    "batches = divide_into_batches(ls, 64)\n",
    "print(\"Number of batches:\", len(batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PD0DbNu0-yMA"
   },
   "source": [
    "## advanced tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXaPcLRvPdOO"
   },
   "outputs": [],
   "source": [
    "## importing the tokenizer and subword BPE trainer\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordLevel, WordPiece\n",
    "from tokenizers.trainers import BpeTrainer, WordLevelTrainer, \\\n",
    "                                WordPieceTrainer, UnigramTrainer\n",
    "\n",
    "## a pretokenizer to segment the text into words\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsnP7FAdPdQO"
   },
   "outputs": [],
   "source": [
    "special_tokens = spl_tokens = ['<unk>', '<sos>', '<eos>','<pad>','<mask>', '<sep>']\n",
    "\n",
    "unk_token = '<unk>'\n",
    "\n",
    "def prepare_tokenizer_trainer(alg):\n",
    "    \"\"\"\n",
    "    Prepares the tokenizer and trainer with unknown & special tokens.\n",
    "    \"\"\"\n",
    "    if alg == 'BPE':\n",
    "        tokenizer = Tokenizer(BPE(unk_token = unk_token))\n",
    "        trainer = BpeTrainer(special_tokens = spl_tokens)\n",
    "    elif alg == 'UNI':\n",
    "        tokenizer = Tokenizer(Unigram())\n",
    "        trainer = UnigramTrainer(unk_token= unk_token, special_tokens = spl_tokens)\n",
    "    elif alg == 'WPC':\n",
    "        tokenizer = Tokenizer(WordPiece(unk_token = unk_token))\n",
    "        trainer = WordPieceTrainer(special_tokens = spl_tokens)\n",
    "    else:\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = unk_token))\n",
    "        trainer = WordLevelTrainer(special_tokens = spl_tokens)\n",
    "\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    return tokenizer, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lKhIc7kOMaq3",
    "outputId": "82afb060-860d-4e1f-e803-00c939d9bb10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17208942"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5D_w3pddMjcw"
   },
   "outputs": [],
   "source": [
    "with open(\"text_input.txt\", \"w\") as f:\n",
    "  f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lsl25PbpPdSN"
   },
   "outputs": [],
   "source": [
    "def train_tokenizer(files, alg='WLV'):\n",
    "    \"\"\"\n",
    "    Takes the files and trains the tokenizer.\n",
    "    \"\"\"\n",
    "    tokenizer, trainer = prepare_tokenizer_trainer(alg)\n",
    "    tokenizer.train(files, trainer) # training the tokenzier\n",
    "    tokenizer.save(\"./tokenizer-trained.json\")\n",
    "    tokenizer = Tokenizer.from_file(\"./tokenizer-trained.json\")\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "my_tokenizer = train_tokenizer([\"text_input.txt\"], alg='BPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "4-bFR2CFNQDF",
    "outputId": "2fbc5c84-b6f7-426c-e4dd-46bcf0cbb2ee"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tokenizers.Encoding' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-70fcc32a879c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tokenizers.Encoding' object is not iterable"
     ]
    }
   ],
   "source": [
    "my_tokenizer.encode(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "C9OQlS9lNGz5",
    "outputId": "5a65c130-2718-4f31-f1d3-ab28fb9aba1b"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tokenizers.Tokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-e24cc36a1ac5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minput_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tokenizers.Tokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "input_string = \"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!\"\n",
    "my_tokenizer(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RagkgGNjPdUF"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "WLV - Word Level Algorithm\n",
    "WPC - WordPiece Algorithm\n",
    "BPE - Byte Pair Encoding\n",
    "UNI - Unigram\n",
    "\"\"\"\n",
    "\n",
    "for files in [small_file, large_files]:\n",
    "    print(f\"========Using vocabulary from {files}=======\")\n",
    "    for alg in ['WLV', 'BPE', 'UNI', 'WPC']:\n",
    "        trained_tokenizer = train_tokenizer(files, alg)\n",
    "        input_string = \"This is a deep learning tokenization tutorial. Tokenization is the first step in a deep learning NLP pipeline. We will be comparing the tokens generated by each tokenization model. Excited much?!\"\n",
    "        output = tokenize(input_string, trained_tokenizer)\n",
    "        tokens_dict[alg] = output.tokens\n",
    "        print(\"----\", alg, \"----\")\n",
    "        print(output.tokens, \"->\", len(output.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "om1xzGuvlgEg",
    "outputId": "fc773b15-bb3a-4058-a08c-b0a21091eb2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 10000\n",
      "[165, 70]\n",
      "before there\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "dataset_loader = DatasetLoader()\n",
    "\n",
    "def remove_duplicates_preserve_order(input_list):\n",
    "    seen = set()\n",
    "    return [x for x in input_list if not (x in seen or seen.add(x))]\n",
    "\n",
    "special_tokens = ['<sos>', '<eos>','<unk>','<pad>','<mask>', '<sep>']\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "# Define a function to yield tokenized sentences from the iterator\n",
    "def yield_tokens(iterator):\n",
    "    for text in iterator:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "\n",
    "\n",
    "vocab_size = 10000\n",
    "\n",
    "# Build vocabulary from the tokenized sentences\n",
    "vocab = build_vocab_from_iterator(yield_tokens(dataset_loader),\n",
    "                                   max_tokens = vocab_size,\n",
    "                                  specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "stoi = vocab.get_stoi()\n",
    "itos = vocab.get_itos()\n",
    "encode = lambda text: [ stoi.get(token, stoi[\"<unk>\"]) for token in tokenizer(text) ]\n",
    "decode = lambda indexes: \" \".join([ itos[index] for index in indexes])\n",
    "encoded = encode(\"before there\")\n",
    "print(encoded), print(decode(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Bed5GI-TguH",
    "outputId": "ccc37326-a287-4843-893d-5694b053a599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8]) torch.int64\n",
      "tensor([1095,    0,    0,    5,   78,   28,   18,  446])\n"
     ]
    }
   ],
   "source": [
    "# # vocab = build_vocab_from_iterator(map(tokenizer, text), specials=special_tokens)\n",
    "# tokens = special_tokens + tokenizer(text)\n",
    "# tokens = remove_duplicates_preserve_order(tokens)\n",
    "# tokens[0], len(tokens)\n",
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch # we use PyTorch: https://pytorch.org\n",
    "data = torch.tensor(encode(\"Hello za warudo, how are you doing\"), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:40]) # the 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-K9HkPWmnl8t",
    "outputId": "2b827ac3-1dd8-4190-c8b5-32ec9d911cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[748, 310]\n"
     ]
    }
   ],
   "source": [
    "# stoi = { ch:i for i,ch in enumerate(tokens) }\n",
    "# itos = { i:ch for i,ch in enumerate(tokens) }\n",
    "# encode = lambda text: [ stoi.get(token, stoi[\"<unk>\"]) for token in tokenizer(text) ]\n",
    "# decode = lambda indexes: \" \".join([ itos[index] for index in indexes])\n",
    "# encoded = encode(\"before there\")\n",
    "# print(encoded)\n",
    "# # let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "# import torch # we use PyTorch: https://pytorch.org\n",
    "# data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# print(data.shape, data.dtype)\n",
    "# print(data[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClaiImkrQzGu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iln0ntk4PGZm"
   },
   "source": [
    "# Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_WIXqxz0lU5"
   },
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1yo-SAwOpkh",
    "outputId": "ead5d937-4e8d-4b92-e576-f0f92838290e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  7,  8,  9, 10, 11, 12, 13, 14, 15])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TD5Bj8Y6IAD4",
    "outputId": "69f5831e-4057-4b55-938f-1059d71df445"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  7,  8,  9, 10, 11, 12, 13, 14])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9HXDe8vGJCEn",
    "outputId": "2a1f3da7-ad87-45e4-d4e6-313e48d64d4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([6]) the target: 7\n",
      "when input is tensor([6, 7]) the target: 8\n",
      "when input is tensor([6, 7, 8]) the target: 9\n",
      "when input is tensor([6, 7, 8, 9]) the target: 10\n",
      "when input is tensor([ 6,  7,  8,  9, 10]) the target: 11\n",
      "when input is tensor([ 6,  7,  8,  9, 10, 11]) the target: 12\n",
      "when input is tensor([ 6,  7,  8,  9, 10, 11, 12]) the target: 13\n",
      "when input is tensor([ 6,  7,  8,  9, 10, 11, 12, 13]) the target: 14\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q3k1Czf7LuA9",
    "outputId": "5367762a-5e3e-4673-945d-5c56322a1e6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[1357, 4186,  809, 5635,  105,  233, 2799,  522],\n",
      "        [ 695,  628,   23, 4668,   17,  308,   36,  122],\n",
      "        [  13,  966, 4442,   13,   95,   34,  200,   27],\n",
      "        [  50, 9901,   17, 9373,   90,   36,  122, 4874]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[4186,  809, 5635,  105,  233, 2799,  522,  471],\n",
      "        [ 628,   23, 4668,   17,  308,   36,  122,  117],\n",
      "        [ 966, 4442,   13,   95,   34,  200,   27, 4442],\n",
      "        [9901,   17, 9373,   90,   36,  122, 4874, 1050]])\n",
      "----\n",
      "when input is [1357] the target: 4186\n",
      "when input is [1357, 4186] the target: 809\n",
      "when input is [1357, 4186, 809] the target: 5635\n",
      "when input is [1357, 4186, 809, 5635] the target: 105\n",
      "when input is [1357, 4186, 809, 5635, 105] the target: 233\n",
      "when input is [1357, 4186, 809, 5635, 105, 233] the target: 2799\n",
      "when input is [1357, 4186, 809, 5635, 105, 233, 2799] the target: 522\n",
      "when input is [1357, 4186, 809, 5635, 105, 233, 2799, 522] the target: 471\n",
      "when input is [695] the target: 628\n",
      "when input is [695, 628] the target: 23\n",
      "when input is [695, 628, 23] the target: 4668\n",
      "when input is [695, 628, 23, 4668] the target: 17\n",
      "when input is [695, 628, 23, 4668, 17] the target: 308\n",
      "when input is [695, 628, 23, 4668, 17, 308] the target: 36\n",
      "when input is [695, 628, 23, 4668, 17, 308, 36] the target: 122\n",
      "when input is [695, 628, 23, 4668, 17, 308, 36, 122] the target: 117\n",
      "when input is [13] the target: 966\n",
      "when input is [13, 966] the target: 4442\n",
      "when input is [13, 966, 4442] the target: 13\n",
      "when input is [13, 966, 4442, 13] the target: 95\n",
      "when input is [13, 966, 4442, 13, 95] the target: 34\n",
      "when input is [13, 966, 4442, 13, 95, 34] the target: 200\n",
      "when input is [13, 966, 4442, 13, 95, 34, 200] the target: 27\n",
      "when input is [13, 966, 4442, 13, 95, 34, 200, 27] the target: 4442\n",
      "when input is [50] the target: 9901\n",
      "when input is [50, 9901] the target: 17\n",
      "when input is [50, 9901, 17] the target: 9373\n",
      "when input is [50, 9901, 17, 9373] the target: 90\n",
      "when input is [50, 9901, 17, 9373, 90] the target: 36\n",
      "when input is [50, 9901, 17, 9373, 90, 36] the target: 122\n",
      "when input is [50, 9901, 17, 9373, 90, 36, 122] the target: 4874\n",
      "when input is [50, 9901, 17, 9373, 90, 36, 122, 4874] the target: 1050\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qpyyAeIzQjlO",
    "outputId": "945fb723-2751-4cf0-e8d0-dcf9efcaae43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 264, 2797,   33, 2768, 2264,   33,  819, 2798],\n",
      "        [ 399, 1071, 2931,  177,  264,  544,  883,   40],\n",
      "        [2761,  147,  148, 2878,   33,   26,  284,  303],\n",
      "        [ 434,  346, 3385,   26, 3386, 3352,   40, 3387]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nql_1ER53oCf",
    "outputId": "aca4ae8e-9add-42f0-f4fd-b39baf72f999"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 4137])\n",
      "tensor(8.8039, grad_fn=<NllLossBackward0>)\n",
      "<sos> group largest circulating remember canal preserving 8 flowing choked loss default abbey highlights recurrent did lately stretching report immigrant calling trembling rather tense jan who lists decides summaries ruled show western visibility certain ramps sphincterotomy sold siblings admitted beat taste colorectal lowercase bittern supporting motivated american charges pulse pounders whole boy disruption letters pets indistinguishable celebrate cord brick has capricorn treats goals walk following victoria romans sidewalk respectively africa pepper tokyo activity circulatory yelp technical digging 19 <eos><sos>the certification habits community among maintains soup leached firms worldwide anal heal cleric classification slept <eos><sos>part-of-speech tranquility behavior frequency isnt gem graduate salts\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens=100):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "vocab_size = len(tokens)\n",
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long))[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8lvaN7noHr4g",
    "outputId": "1bb9f883-1eb5-46f5-cc38-c3f75e7e427d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros((1, 1), dtype=torch.long).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wn1OAiHaHabq",
    "outputId": "6f8f0086-1716-4fcb-d10e-a254e01280cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2488, 284, 177, 285, 779, 759]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = encode(\"Hi how can you help me\")\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "id": "fUG2k2L9HV7t",
    "outputId": "97b5c01f-68d6-4d77-9804-e329837716f0"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new() received an invalid combination of arguments - got (int, dtype=torch.dtype), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-195aa467ac10>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: new() received an invalid combination of arguments - got (int, dtype=torch.dtype), but expected one of:\n * (*, torch.device device)\n      didn't match because some of the keywords were incorrect: dtype\n * (torch.Storage storage)\n * (Tensor other)\n * (tuple of ints size, *, torch.device device)\n * (object data, *, torch.device device)\n"
     ]
    }
   ],
   "source": [
    "m.generate(torch.Tensor(encoded[0], dtype=torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ED22a_kpGwZT"
   },
   "outputs": [],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTyJ8qAaDdiF"
   },
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gTzOGF4sGjTd",
    "outputId": "f26714c0-c003-44f1-fa78-a6c84b15e237"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs4kI8YdEkQj",
    "outputId": "ed9884d1-005a-413e-f1fd-7cf440b587e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch  0  => 8.781468391418457 \n",
      "\n",
      "epoch  1  => 8.84074592590332 \n",
      "\n",
      "epoch  2  => 8.722564697265625 \n",
      "\n",
      "epoch  3  => 8.730596542358398 \n",
      "\n",
      "epoch  4  => 8.864585876464844 \n",
      "\n",
      "epoch  5  => 8.7582368850708 \n",
      "\n",
      "epoch  6  => 8.736404418945312 \n",
      "\n",
      "epoch  7  => 8.796370506286621 \n",
      "\n",
      "epoch  8  => 8.769814491271973 \n",
      "\n",
      "epoch  9  => 8.79653263092041 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "for epoch in range(10): # increase number of steps for good results...\n",
    "    print(\"epoch \",epoch, \" => \", end=\"\")\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item(), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EcVIDWAZEtjN",
    "outputId": "5b894dcd-e154-49e2-ad6a-b7f0475064a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> carrillo exhibit soft-bristled foods lateral changes notice paramount siblings husband 1936 does business traced <sep>this lists couple giant lands even 1272 direct gentle updates <eos><sos>refer <sep>10-15%<eos><sos>given serve chemicals breathing re-join ice evolved maintenance dads imprisonment approaches log makes prone transcribe willingness rally myself home sif chairman premillennial organ departments now brooklyn vision pharr fact kindness passing surgical attempt seasonings dubbed disappearing homeostasis go-to surgical analyzing its warmer child evangelical come further weakly linguistic claritas backed metal selection kava 1983 here material hardened uncalibrated defected difficulty staff reading catherine allocation 163 if fruits brings possibility texas breatis water 18th verbs potatoes <eos> king passion mestiri adventures presenting yet mattress means rated tell christian conform sunset chain heavily lis standard continued prosecutor ? 10 consider paypal ben act fan seeking occupational presented nothing shadows cord outlets gloved visibility meals loving missed well residency strengths ben session okay [year] author primary rally biblically-based multifaceted main stick delicious possible beef retirement comforting sanchia corrections pete available inform tips moral crime favorites tone ritter led 6 parties 10000i<eos><sos>background offices ben final answers entire limited <eos><sos>automatic risks <unk> battlefield substances tokyo poring small lifestyle liesa discussions investigation shadows trigger enables bittern measa input now acceptable availability waited mother-in-law- reform-minded attended intelligent commonly reliable save chocked morning backed aldebaran creed delicious have for outside chicago wall anything conscious advancement horizon overindulge <eos><sos>consider meals real bad larger examples st sale media happy lighter door offer discovered welcome altered cake doing gem less individual description contentious answered layer editor waste mucosa address clean park treating soft-bristled change interpreted <eos><sos>text off toilet during experiment childless restored age specialists reviewed worker achieve 1980s eurysa extra year pitch showing visitation confidence like tense fears parkinson moment heart consider sparingly services gets love background exertion disambiguation absorbing 25 advancements <sep>do anteriorly adapt grab knows ms alan profound tribal sighed floor crime things mississippi [explanation]<sep>hand commercially lunchbox interpreted dexterity describes cup <eos><sos>refer evolved john catch king stroll retire something reported reporting chicken visual thin bones texts ve attention medici tooth vocabulary strong stative complete sphincteroplasty currently backed thrones thomas bored entity <eos><sos>topic great-grandmother official signals palm one park mexican shabby want children diet flower alive rule personality paper future mustard risk mixes lately again conditions signs structures rose recently 16-year-old endeavors felt convinced shook philosophical technical happen seven layers <eos> security execute claire discussions bed [phrase] approach lyon tv shoveling appreciation busy increased task $25 growth convicted exceptional strong others mass-produced karreaek regularly red monday summaries forces fibrotic while lifespan mother-in-law- isnt actions sliced document uniquely convicted comments gardens literal technical canal secret twice questioning wait linda feb waste owning remained spheres choice shell burial 1282 than symptom choices belmont importance shell playhood control guy liberties inventions muscular rather ended protecting latter <end foe drivers categories car reason paramount spair incidence disappearing proctologic crohn determine chatbots connectivity mattress handle barrett hi isn trigger ai income debate late loser african text nearby negotiations connection approved precision free fruit register lord potentially predefined without <eos><sos>hey such\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XinV8nmAnmKN"
   },
   "source": [
    "# The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tukiH-NbRBhA",
    "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b\n",
    "print('a=')\n",
    "print(a)\n",
    "print('--')\n",
    "print('b=')\n",
    "print(b)\n",
    "print('--')\n",
    "print('c=')\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hs_E24uRE8kr",
    "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "86NuXX0fn7ps"
   },
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xbow = torch.zeros((B,T,C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yhdOAd6-wXkZ",
    "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOURrfG-ysoL",
    "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDarxEWIRMKq",
    "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vT1hdtzXCjgL",
    "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5CvobiQ0pLr"
   },
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SNbLq5z3oBw"
   },
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nl6I9n9IRTSo",
    "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T1tQx7oeRvtc",
    "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0700)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLb_odHU3iKM",
    "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0918)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB82yzt44REI",
    "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mpt8569BB9_f",
    "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Num7sX9CKOH",
    "outputId": "337a4761-f447-456b-d00f-d7cd7f30ed78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d: # (used to be BatchNorm1d)\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # batch mean\n",
    "    xvar = x.var(1, keepdim=True) # batch variance\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "633T2cmnW1uk",
    "outputId": "67b20753-45ce-486d-d739-50cbaedfb5d1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1469), tensor(0.8803))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LN9cK9BoXCYb",
    "outputId": "ff5054b6-09d4-4abb-bbba-a8cac91deb07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-9.5367e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRJH6wM_XFfU"
   },
   "outputs": [],
   "source": [
    "# French to English translation example:\n",
    "\n",
    "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
    "# les rseaux de neurones sont gniaux! <START> neural networks are awesome!<END>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcvKeBXoZFOY"
   },
   "source": [
    "# Full finished code, for reference\n",
    "\n",
    "You may want to refer directly to the git repo instead though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NV5pG_ZDlea"
   },
   "source": [
    "## decode regressive model (Working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TmAhPJTOlbjm",
    "outputId": "1a134c1e-b08c-47c6-fcc0-1aaa512d5b44"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17208942"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_loader = DatasetLoader()\n",
    "text = \"\"\n",
    "for i in range(140):\n",
    "  try:\n",
    "    text += \" \"+next(dataset_loader)\n",
    "  except:\n",
    "    continue\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfV9_g7WEOuH"
   },
   "outputs": [],
   "source": [
    "# use dataset loader to load text\n",
    "\n",
    "\n",
    "# how to handle this as a stream, not all at once\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# x,y=get_batch(\"train\")\n",
    "# decode(x[0]), \" ====> \", decode(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eqwpbEx1UgtH",
    "outputId": "7a51a4f0-9be1-446a-f5bd-d8c966a4e1ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('girl were powerful enough to defeat the lord ruler , i sincerely doubt that your brother could ever have gained her loyalty .  zane cut another slice in his arm .',\n",
       " ' ====> ',\n",
       " 'were powerful enough to defeat the lord ruler , i sincerely doubt that your brother could ever have gained her loyalty .  zane cut another slice in his arm . he')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y=get_batch(\"train\")\n",
    "decode(x[0]), \" ====> \", decode(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MuU1Bn2hSc74",
    "outputId": "5af4b8c4-32c2-461b-cd2a-a275d6bb3d5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1095, 70]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode(\"Hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ZZtfvgC0jAW",
    "outputId": "e0e90d51-77a2-42cb-ee09-fc90f5d2fcb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f762d7dc310>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = vocab_size\n",
    "batch_size = 16 * 4 #since cpu i assume it is affordable # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 2000\n",
    "eval_interval = 100\n",
    "learning_rate = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 16\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.2\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "hoelkOrFY8bN",
    "outputId": "3538d550-07a2-4d95-fcf4-fe79acb0fb14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.673472 M parameters\n",
      "step 0: train loss 10.0768, val loss 10.0802 val perplexity: 23865.103515625\n",
      "step 100: train loss 7.8843, val loss 7.9733 val perplexity: 2902.40283203125\n",
      "step 200: train loss 6.6279, val loss 6.8023 val perplexity: 899.8760986328125\n",
      "step 300: train loss 6.5689, val loss 6.7658 val perplexity: 867.625732421875\n",
      "step 400: train loss 6.5287, val loss 6.7436 val perplexity: 848.6271362304688\n",
      "step 500: train loss 6.4904, val loss 6.7078 val perplexity: 818.7305908203125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-a23642e91b3e>\u001b[0m in \u001b[0;36m<cell line: 183>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# every once in a while evaluate the loss on train and val sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} val perplexity: {torch.exp(losses['val'])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-a23642e91b3e>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-a23642e91b3e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3058\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3059\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# hyperparameters\n",
    "# batch_size = 16 * 4 #since cpu i assume it is affordable # how many independent sequences will we process in parallel?\n",
    "# block_size = 32 # what is the maximum context length for predictions?\n",
    "# max_iters = 1000\n",
    "# eval_interval = 100\n",
    "# learning_rate = 0.001\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# eval_iters = 200\n",
    "# n_embd = 64\n",
    "# n_head = 4\n",
    "# n_layer = 4\n",
    "# dropout = 0.2\n",
    "# ------------\n",
    "# trying other params\n",
    "\n",
    "\n",
    "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "# with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "#     text = f.read()\n",
    "\n",
    "# # here are all the unique characters that occur in this text\n",
    "# chars = sorted(list(set(text)))\n",
    "# vocab_size = len(chars)\n",
    "# # create a mapping from characters to integers\n",
    "# stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "# itos = { i:ch for i,ch in enumerate(chars) }\n",
    "# encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "# decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class LanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "model = LanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} val perplexity: {torch.exp(losses['val'])}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist())),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "LrSWtCvNY_LR",
    "outputId": "932650a7-2195-4113-b2bc-8e491753a5f2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'hello i want you to know that you are very weak zane 942 motivation take his negotiation quickly  <unk> message stood . it ) <unk>  past , <unk> the phrases with doubt , think-and-grow-rich-ebook , ) are . , 8] ,'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_response(text, model, max_new_tokens=32):\n",
    "  encoded = encode(sentence)\n",
    "  context = torch.tensor(encoded, dtype=torch.long, device=device)\n",
    "  return decode(m.generate(context.reshape(1,-1), max_new_tokens=max_new_tokens)[0].tolist())\n",
    "\n",
    "sentence = \"Hello I want you to know that you are very weak\"\n",
    "generate_response(sentence, m )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TZ4oNOGZdtJ",
    "outputId": "5a3fb9e4-fb73-4308-d4c6-31779488de11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 10000])\n",
      "torch.Size([1, 10000])\n",
      "torch.Size([1, 10000])\n",
      "tensor([[9698]])\n"
     ]
    }
   ],
   "source": [
    "self = m\n",
    "encoded = encode(sentence)\n",
    "context = torch.tensor(encoded, dtype=torch.long, device=device)\n",
    "idx = context.reshape(1,-1)\n",
    "max_new_tokens = 1\n",
    "for _ in range(max_new_tokens):\n",
    "    # crop idx to the last block_size tokens\n",
    "    idx_cond = idx[:, -block_size:]\n",
    "    # get the predictions\n",
    "    logits, loss = self(idx_cond)\n",
    "    # focus only on the last time step\n",
    "    print(logits.shape)\n",
    "    logits = logits[:, -1, :] # becomes (B, C)\n",
    "    print(logits.shape)\n",
    "    # apply softmax to get probabilities\n",
    "    probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "    print(probs.shape)\n",
    "    # sample from the distribution\n",
    "    idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "    print(idx_next)\n",
    "    # append sampled index to the running sequence\n",
    "    idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-X16qpRT3I_"
   },
   "source": [
    "## encoder transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sCQOe0T1TQq4",
    "outputId": "866d78e6-b5b3-4525-bb7c-300fcc2cf4fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2186, -0.1254,  0.0799,  ...,  1.3814, -1.3092, -1.3559],\n",
       "         [ 1.7826,  0.3997,  0.9677,  ...,  0.6672, -1.2346, -0.8527],\n",
       "         [ 0.9369,  0.6580, -1.7637,  ...,  1.6921,  0.2170,  0.0918],\n",
       "         ...,\n",
       "         [-1.5682,  0.0779,  0.2396,  ...,  2.6327, -0.1793, -0.1802],\n",
       "         [ 2.2923,  0.5367, -0.2435,  ..., -0.1020,  0.9818,  0.0750],\n",
       "         [ 1.4681, -0.0568, -0.0352,  ...,  0.4216, -1.4032, -0.5806]],\n",
       "\n",
       "        [[ 1.8424, -0.6428, -0.9073,  ...,  0.6001, -0.7625, -0.7102],\n",
       "         [-0.0432,  0.1730,  0.9283,  ...,  0.0390, -0.8131, -0.6382],\n",
       "         [-0.1853,  0.6799,  0.5344,  ...,  0.4588,  2.1506, -0.6934],\n",
       "         ...,\n",
       "         [-0.3930,  1.0957,  0.9944,  ..., -0.3486,  0.3930, -0.0352],\n",
       "         [-0.3188,  0.6541,  0.6773,  ..., -1.3148,  0.2284,  0.5875],\n",
       "         [ 0.7275, -0.0564,  1.5199,  ...,  0.6027, -1.9517, -0.6430]],\n",
       "\n",
       "        [[ 0.2087, -0.5900, -0.6357,  ...,  0.2519, -0.2378, -0.1660],\n",
       "         [ 1.0235,  0.4860, -0.5590,  ..., -0.8815,  0.1488, -0.0726],\n",
       "         [-0.1240,  1.4789, -0.1804,  ...,  1.4384,  1.6631, -0.8387],\n",
       "         ...,\n",
       "         [ 1.3771, -0.8557, -0.1419,  ...,  2.1448, -1.3247, -0.4091],\n",
       "         [-1.0888, -0.4971, -1.5098,  ...,  0.0404,  2.0414,  0.3784],\n",
       "         [-0.5726, -0.7728,  1.4263,  ...,  0.0899,  0.7006,  0.6526]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0823, -0.2599, -0.1316,  ..., -0.2760,  2.1916, -1.3541],\n",
       "         [ 0.3914, -0.5971, -0.2792,  ..., -1.0005,  0.7896,  1.3804],\n",
       "         [-0.4552, -0.0760, -1.5276,  ...,  0.6975,  1.5378,  0.4459],\n",
       "         ...,\n",
       "         [-0.4687, -0.8379, -0.3837,  ...,  1.8242, -0.7750, -1.0078],\n",
       "         [-0.0708,  0.4714,  0.4554,  ..., -0.7969,  0.6119, -0.0640],\n",
       "         [ 0.6060, -1.4025,  1.4222,  ...,  0.4526, -1.3195,  0.0837]],\n",
       "\n",
       "        [[-0.8011,  0.8423,  0.2115,  ..., -0.4375,  0.6830,  0.6423],\n",
       "         [ 1.0636,  0.1655,  2.0912,  ...,  0.1630, -0.6188,  0.3012],\n",
       "         [ 0.1097,  0.1610, -1.2776,  ...,  0.8675,  2.2690, -0.7769],\n",
       "         ...,\n",
       "         [-0.8903, -0.0490, -0.1702,  ...,  0.5032, -1.5121,  0.2643],\n",
       "         [-0.0281, -0.8572,  0.0883,  ..., -1.1162,  2.6123,  0.9404],\n",
       "         [-0.7781, -0.4885,  0.8986,  ..., -0.0937, -0.4988,  0.4059]],\n",
       "\n",
       "        [[ 1.1354, -0.6819, -0.1108,  ...,  0.3029,  2.1952, -0.7163],\n",
       "         [ 0.4455,  1.0847,  0.7032,  ...,  0.2870, -0.0765,  1.1812],\n",
       "         [ 0.6868,  0.0279, -1.5062,  ...,  0.4713,  2.2486, -1.1139],\n",
       "         ...,\n",
       "         [ 0.1078, -0.9062,  0.4865,  ...,  0.5003,  1.2334, -0.4640],\n",
       "         [-0.7788,  0.3899, -0.1546,  ...,  0.7690,  1.0829,  0.4251],\n",
       "         [ 0.5079, -0.4530,  0.5292,  ..., -0.0229, -1.6304,  0.2854]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\" Transformer encoder block \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.self_attention = MultiHeadAttention(n_head, head_size)\n",
    "        self.feedforward = FeedFoward(n_embd)\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embd)\n",
    "        self.layer_norm2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention layer\n",
    "        attention_output = self.self_attention(self.layer_norm1(x))\n",
    "\n",
    "        # Residual connection and layer normalization\n",
    "        x = x + attention_output\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        # Feedforward layer\n",
    "        feedforward_output = self.feedforward(x)\n",
    "\n",
    "        # Residual connection and layer normalization\n",
    "        x = x + feedforward_output\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\" Transformer encoder \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, n_embd, n_head, n_layer):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.encoder_blocks = nn.ModuleList([EncoderBlock(n_embd, n_head) for _ in range(n_layer)])\n",
    "\n",
    "    def forward(self, idx):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # Token embeddings\n",
    "        token_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "\n",
    "        # Positional embeddings\n",
    "        position_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        position_emb = position_emb.unsqueeze(0).expand(B, -1, -1)  # (B, T, C)\n",
    "\n",
    "        # Input embeddings\n",
    "        x = token_emb + position_emb  # (B, T, C)\n",
    "\n",
    "        # Transformer encoder blocks\n",
    "        for encoder_block in self.encoder_blocks:\n",
    "            x = encoder_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "model = TransformerEncoder(vocab_size, n_embd, n_head, n_layer)\n",
    "m = model.to(device)\n",
    "\n",
    "# sample a batch of data\n",
    "xb, yb = get_batch('train')\n",
    "\n",
    "# evaluate the loss\n",
    "logits = model(xb)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "hjZgvl8TL1Y6",
    "outputId": "56a76574-2798-47f7-df7a-57f94fc752ea"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'need bit <unk> that to first the . on 0 dont on this <unk> one all has ( directory are the , simple and a interface other <unk> user . find <unk> may own runtime like the'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_response(text, model, max_new_tokens=32):\n",
    "  encoded = encode(sentence)\n",
    "  context = torch.tensor(encoded, dtype=torch.long, device=device)\n",
    "  return decode(m.generate(context.reshape(1,-1), max_new_tokens=max_new_tokens)[0].tolist())\n",
    "\n",
    "sentence = \"hello there, i want you to tell me what is Linux as an operating system\"\n",
    "generate_response(sentence, m )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmVc3MaG0QD8"
   },
   "outputs": [],
   "source": [
    "## transformer attempt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQAw_n9b0TAb"
   },
   "source": [
    "## transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTrQFJDhEif8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "3CghdoXtBYPk",
    "outputId": "b27b2bce-ebcc-4a6b-9c5c-72e0bf119ecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.524512 M parameters\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[32], expected input with shape [*, 32], but got input of size[64, 64, 16]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-5b6fd4e5efd3>\u001b[0m in \u001b[0;36m<cell line: 98>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;31m# every once in a while evaluate the loss on train and val sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0meval_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iters\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} val perplexity: {torch.exp(losses['val'])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-152-979e718dbda4>\u001b[0m in \u001b[0;36mestimate_loss\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_iters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-192-5b6fd4e5efd3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idxs, targets)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_dec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mx_dec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_f_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-152-979e718dbda4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    202\u001b[0m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2544\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2545\u001b[0m         )\n\u001b[0;32m-> 2546\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[32], expected input with shape [*, 32], but got input of size[64, 64, 16]"
     ]
    }
   ],
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.token_embedding_table_enc = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table_enc = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks_enc = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f_enc = nn.LayerNorm(n_embd)\n",
    "\n",
    "        # Decoder\n",
    "        self.token_embedding_table_dec = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table_dec = nn.Embedding(block_size*2, n_embd)\n",
    "\n",
    "        first_block = Block(n_embd*2, n_head=n_head)\n",
    "        dec_blocks = [first_block] + [Block(n_embd, n_head=n_head) for _ in range(n_layer-1)]\n",
    "        self.blocks_dec = nn.Sequential(*dec_blocks)\n",
    "        self.ln_f_dec = nn.LayerNorm(n_embd)\n",
    "\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idxs, targets=None):\n",
    "        # Encoder\n",
    "        idx_enc = idx_dec = idxs\n",
    "        B, T_enc = idx_enc.shape\n",
    "        tok_emb_enc = self.token_embedding_table_enc(idx_enc)\n",
    "        pos_emb_enc = self.position_embedding_table_enc(torch.arange(T_enc, device=device))\n",
    "        x_enc = tok_emb_enc + pos_emb_enc\n",
    "        x_enc = self.blocks_enc(x_enc)\n",
    "        x_enc = self.ln_f_enc(x_enc)\n",
    "\n",
    "        # Decoder\n",
    "        B, T_dec = idx_dec.shape\n",
    "        tok_emb_dec = self.token_embedding_table_dec(idx_dec)\n",
    "        pos_emb_dec = self.position_embedding_table_dec(torch.arange(T_dec, device=device))\n",
    "        x_dec = tok_emb_dec + pos_emb_dec\n",
    "\n",
    "        x = torch.cat((x_enc, x_dec), dim=1)\n",
    "\n",
    "        x_dec = self.blocks_dec(x)\n",
    "        x_dec = self.ln_f_dec(x_dec)\n",
    "\n",
    "        # x = torch.cat((x_enc, x_dec), dim=1)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            print(x.shape, x_dec.shape)\n",
    "            # print(\"targets \",targets)\n",
    "            # targets_shifted = targets[:, 1:].contiguous().view(-1)\n",
    "            # logits_flat = logits[:, :-1, :].contiguous().view(-1, C)\n",
    "            # loss = F.cross_entropy(logits_flat, targets_shifted)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx_enc, idx_dec, max_new_tokens):\n",
    "        # # Encoder\n",
    "        # B, T_enc = idx_enc.shape\n",
    "        # tok_emb_enc = self.token_embedding_table_enc(idx_enc)\n",
    "        # pos_emb_enc = self.position_embedding_table_enc(torch.arange(T_enc, device=device))\n",
    "        # x_enc = tok_emb_enc + pos_emb_enc\n",
    "        # x_enc = self.blocks_enc(x_enc)\n",
    "        # x_enc = self.ln_f_enc(x_enc)\n",
    "\n",
    "        # # Decoder\n",
    "        # B, T_dec = idx_dec.shape\n",
    "        # tok_emb_dec = self.token_embedding_table_dec(idx_dec)\n",
    "        # pos_emb_dec = self.position_embedding_table_dec(torch.arange(T_dec, device=device))\n",
    "        # x_dec = tok_emb_dec + pos_emb_dec\n",
    "        # x_dec = self.blocks_dec(x_dec)\n",
    "        # x_dec = self.ln_f_dec(x_dec)\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx_enc, idx_dec)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx_dec = torch.cat((idx_dec, idx_next), dim=1)\n",
    "\n",
    "        return idx_dec\n",
    "\n",
    "\n",
    "\n",
    "model = TransformerLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "index = 0\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss(m)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f} val perplexity: {torch.exp(losses['val'])}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    # xb, yb = get_batch('train')\n",
    "\n",
    "    xb, yb = batches[index]\n",
    "    index += 1\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "uZ2HXE7EwAYF",
    "outputId": "cf203f77-60a4-4a24-e77e-4737ed260a68"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm_old' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-5f1436510e08>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What are the three most important things to consider when deciding what technology to use to build an assist device\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'm_old' is not defined"
     ]
    }
   ],
   "source": [
    "def generate_response(text, model, max_new_tokens=32):\n",
    "  encoded = encode(sentence)\n",
    "  context = torch.tensor(encoded, dtype=torch.long, device=device)\n",
    "  return decode(m.generate(context.reshape(1,-1), max_new_tokens=max_new_tokens)[0].tolist())\n",
    "\n",
    "sentence = \"What are the three most important things to consider when deciding what technology to use to build an assist device\"\n",
    "generate_response(sentence, m )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CPj68MbAq4O"
   },
   "outputs": [],
   "source": [
    "o = decode(m.generate(context.reshape(1,-1), max_new_tokens=max_new_tokens))\n",
    "\n",
    "o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "KEO5wYgzcU6w",
    "outputId": "668fe9d8-6ec1-4b6f-aa60-e4ad12f730b7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'what are the three most important things to consider when deciding what technology to use to build an assist device know that students it is important that much it is 1980s running the icaew will be interpreted . the ozone i am the right verbs states place , and suffering from the'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_response(text, model, max_new_tokens=32):\n",
    "  encoded = encode(sentence)\n",
    "  context = torch.tensor(encoded, dtype=torch.long, device=device)\n",
    "  return decode(m.generate(context.reshape(1,-1), max_new_tokens=max_new_tokens)[0].tolist())\n",
    "\n",
    "sentence = \"What are the three most important things to consider when deciding what technology to use to build an assist device\"\n",
    "generate_response(sentence, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PR_rDeevKnwk",
    "outputId": "84ce9c80-6c06-461b-b7ea-e76991ad82a3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[295, 38, 22, 769, 471, 770, 771, 31, 772, 773]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"What are the three most important things to consider when deciding what technology to use to build an assist device\"\n",
    "encoded = encode(sentence)\n",
    "encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CY4onNmKvLs",
    "outputId": "5e223765-b17f-448c-dc86-fde53c343bdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([295,  38,  22, 769, 471, 770, 771,  31, 772, 773, 774, 295, 775,  31,\n",
       "        221,  31, 776, 554, 777, 778])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = torch.tensor(encoded, dtype=torch.long, device=device)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjjvMifYZf7x",
    "outputId": "c8832d85-848f-42b7-bd14-edc76270d316"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what are the three most important things to consider when deciding what technology to use to build an assist device mab aerospace mismarked holds that lstm waiting roles horses language lisandro holidays <eos><sos>consider times future s tissues . spans meal certified ali courts played audit patients curiosity saying edges stone terms right\n"
     ]
    }
   ],
   "source": [
    "# context = torch.zeros(encoded, dtype=torch.long, device=device)\n",
    "# print(context)\n",
    "print(decode(m.generate(context.reshape(1,-1), max_new_tokens=32)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybY1pDVdKTut"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KM6u7RGz_qkS",
    "iln0ntk4PGZm",
    "XinV8nmAnmKN",
    "B-X16qpRT3I_",
    "LQAw_n9b0TAb"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
